{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "b68e072c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import operator\n",
        "from pathlib import Path\n",
        "from typing import TypedDict, List, Optional, Literal, Annotated\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.types import Send\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "14738b8a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a9d257c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2cdc68c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 1) Schemas\n",
        "# -----------------------------\n",
        "class Task(BaseModel):\n",
        "    id: int\n",
        "    title: str\n",
        "\n",
        "    goal: str = Field(\n",
        "        ...,\n",
        "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
        "    )\n",
        "    bullets: List[str] = Field(\n",
        "        ...,\n",
        "        min_length=3,\n",
        "        max_length=5,\n",
        "        description=\"3–6 concrete, non-overlapping subpoints to cover in this section.\",\n",
        "    )\n",
        "    target_words: int = Field(\n",
        "        ...,\n",
        "        description=\"Target word count for this section (120–450).\",\n",
        "    )\n",
        "    section_type: Literal[\n",
        "        \"intro\", \"core\", \"examples\", \"checklist\", \"common_mistakes\", \"conclusion\"\n",
        "    ] = Field(\n",
        "        ...,\n",
        "        description=\"Use 'common_mistakes' exactly once in the plan.\",\n",
        "    )\n",
        "\n",
        "class Plan(BaseModel):\n",
        "    blog_title:str\n",
        "    audience:str= Field(...,description=\"Who this blog is for\")\n",
        "    tone: str= Field(...,description=\"What tone the blog should have eg:practical, crisp, technical, etc\")\n",
        "    tasks:List[Task]= Field(description=\"A list of tasks to complete the blog\")\n",
        "\n",
        "class State(TypedDict):\n",
        "    topic: str\n",
        "    plan: Plan\n",
        "    section_results: Annotated[List[str], operator.add]  # reducer appends each worker's output\n",
        "    final_blog: str\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6d41642",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b95ee1ba",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "308eef13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def orchestrator(state:State) -> dict:\n",
        "    \"\"\" Orchestrator function will generate the plan to write the blog \"\"\"\n",
        "    planner = llm.with_structured_output(Plan)\n",
        "    plan = planner.invoke(\n",
        "        [\n",
        "            SystemMessage(\n",
        "                content=(\n",
        "                    \"You are a senior technical writer and developer advocate. Your job is to produce a \"\n",
        "                    \"highly actionable outline for a technical blog post.\\n\\n\"\n",
        "                    \"Hard requirements:\\n\"\n",
        "                    \"- Create 5–7 sections (tasks) that fit a technical blog.\\n\"\n",
        "                    \"- Each section must include:\\n\"\n",
        "                    \"  1) goal (1 sentence: what the reader can do/understand after the section)\\n\"\n",
        "                    \"  2) 3–5 bullets that are concrete, specific, and non-overlapping\\n\"\n",
        "                    \"  3) target word count (120–450)\\n\"\n",
        "                    \"- Include EXACTLY ONE section with section_type='common_mistakes'.\\n\\n\"\n",
        "                    \"Make it technical (not generic):\\n\"\n",
        "                    \"- Assume the reader is a developer; use correct terminology.\\n\"\n",
        "                    \"- Prefer design/engineering structure: problem → intuition → approach → implementation → \"\n",
        "                    \"trade-offs → testing/observability → conclusion.\\n\"\n",
        "                    \"- Bullets must be actionable and testable (e.g., 'Show a minimal code snippet for X', \"\n",
        "                    \"'Explain why Y fails under Z condition', 'Add a checklist for production readiness').\\n\"\n",
        "                    \"- Explicitly include at least ONE of the following somewhere in the plan (as bullets):\\n\"\n",
        "                    \"  * a minimal working example (MWE) or code sketch\\n\"\n",
        "                    \"  * edge cases / failure modes\\n\"\n",
        "                    \"  * performance/cost considerations\\n\"\n",
        "                    \"  * security/privacy considerations (if relevant)\\n\"\n",
        "                    \"  * debugging tips / observability (logs, metrics, traces)\\n\"\n",
        "                    \"- Avoid vague bullets like 'Explain X' or 'Discuss Y'. Every bullet should state what \"\n",
        "                    \"to build/compare/measure/verify.\\n\\n\"\n",
        "                    \"Ordering guidance:\\n\"\n",
        "                    \"- Start with a crisp intro and problem framing.\\n\"\n",
        "                    \"- Build core concepts before advanced details.\\n\"\n",
        "                    \"- Include one section for common mistakes and how to avoid them.\\n\"\n",
        "                    \"- End with a practical summary/checklist and next steps.\\n\\n\"\n",
        "                    \"Output must strictly match the Plan schema.\"\n",
        "                )\n",
        "            ),\n",
        "            HumanMessage(content=f\"topic: {state['topic']}\"),\n",
        "        ]\n",
        "    )\n",
        "    return {\"plan\":plan}\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "63d64079",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fanout(state:State):\n",
        "    \"\"\" this function will call worker nodes required number of times as per the generated plan\"\"\"\n",
        "    return [\n",
        "        Send(\n",
        "            \"worker\", \n",
        "            {'topic': state['topic'], 'task': task, 'plan': state['plan']}\n",
        "        )\n",
        "        for task in state['plan'].tasks\n",
        "    ]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "7800199e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def worker(payload:dict) -> dict:\n",
        "    \"\"\" this function will write the blog section \"\"\"\n",
        "\n",
        "    # Receive the payload from the fanout node (task/plan may be dict if serialized)\n",
        "    topic = payload['topic']\n",
        "    task = payload['task']\n",
        "    plan = payload['plan']\n",
        "    plan_title = plan.blog_title if hasattr(plan, 'blog_title') else plan['blog_title']\n",
        "    task_title = task.title if hasattr(task, 'title') else task['title']\n",
        "    task_goal = task.goal if hasattr(task, 'goal') else task['goal']\n",
        "    task_bullets = task.bullets if hasattr(task, 'bullets') else task['bullets']\n",
        "    \n",
        "    # Generate the blog section\n",
        "    blog_section_markdown = llm.invoke(\n",
        "        [\n",
        "            SystemMessage(\n",
        "                content=(\n",
        "                    \"You are a senior technical writer and developer advocate. Write ONE section of a technical blog post in Markdown.\\n\\n\"\n",
        "                    \"Hard constraints:\\n\"\n",
        "                    \"- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\\n\"\n",
        "                    \"- Stay close to the Target words (±15%).\\n\"\n",
        "                    \"- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\\n\\n\"\n",
        "                    \"Technical quality bar:\\n\"\n",
        "                    \"- Be precise and implementation-oriented (developers should be able to apply it).\\n\"\n",
        "                    \"- Prefer concrete details over abstractions: APIs, data structures, protocols, and exact terms.\\n\"\n",
        "                    \"- When relevant, include at least one of:\\n\"\n",
        "                    \"  * a small code snippet (minimal, correct, and idiomatic)\\n\"\n",
        "                    \"  * a tiny example input/output\\n\"\n",
        "                    \"  * a checklist of steps\\n\"\n",
        "                    \"  * a diagram described in text (e.g., 'Flow: A -> B -> C')\\n\"\n",
        "                    \"- Explain trade-offs briefly (performance, cost, complexity, reliability).\\n\"\n",
        "                    \"- Call out edge cases / failure modes and what to do about them.\\n\"\n",
        "                    \"- If you mention a best practice, add the 'why' in one sentence.\\n\\n\"\n",
        "                    \"Markdown style:\\n\"\n",
        "                    \"- Start with a '## <Section Title>' heading.\\n\"\n",
        "                    \"- Use short paragraphs, bullet lists where helpful, and code fences for code.\\n\"\n",
        "                    \"- Avoid fluff. Avoid marketing language.\\n\"\n",
        "                    \"- If you include code, keep it focused on the bullet being addressed.\\n\"\n",
        "                )\n",
        "            ),\n",
        "            HumanMessage(\n",
        "                content=(\n",
        "                    f\"Blog Title: {plan_title}\\n\"\n",
        "                    f\"topic: {topic}\\n\"\n",
        "                    f\"section: {task_title}\\n\"\n",
        "                    f\"goal: {task_goal}\\n\"\n",
        "                    f\"bullets: {task_bullets}\\n\"\n",
        "                    f\"Return only the section content in Markdown format\"\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    ).content.strip()\n",
        "    \n",
        "    # Must return \"section_results\" (plural) as a list for operator.add reducer to merge\n",
        "    return {\"section_results\": [blog_section_markdown]}\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "44aeecb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def reducer(state:State) -> dict:\n",
        "    \"\"\" this function will reduce the results of the worker nodes into a single final blog \"\"\"\n",
        "    plan = state['plan']\n",
        "    title = plan.blog_title if hasattr(plan, 'blog_title') else plan['blog_title']\n",
        "    section_results = state.get('section_results') or []\n",
        "    body = \"\\n\\n\".join(section_results).strip()\n",
        "    final_markdown = f\"# {title}\\n\\n{body}\"\n",
        "\n",
        "    # Save the final blog to a file\n",
        "    filename=title.lower().replace(\" \",\"_\")+\".md\"\n",
        "    output_path = Path(filename)\n",
        "    output_path.write_text(final_markdown, encoding=\"utf-8\")\n",
        "\n",
        "    return {\"final_blog\":final_markdown}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "e1372219",
      "metadata": {},
      "outputs": [],
      "source": [
        "graph = StateGraph(State)\n",
        "\n",
        "graph.add_node(\"orchestrator\",orchestrator)\n",
        "graph.add_node(\"worker\",worker)\n",
        "graph.add_node(\"reducer\",reducer)\n",
        "\n",
        "graph.add_edge(START, \"orchestrator\")\n",
        "graph.add_conditional_edges(\"orchestrator\",fanout,[\"worker\"])\n",
        "graph.add_edge(\"worker\", \"reducer\")\n",
        "graph.add_edge(\"reducer\", END)\n",
        "\n",
        "app = graph.compile()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "f6437e1c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x10e45cf50>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "ad3f90fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'topic': 'Self-Attention in Transformer Models', 'plan': Plan(blog_title='Understanding Self-Attention in Transformer Models', audience='Developers and Machine Learning Engineers', tone='technical', tasks=[Task(id=1, title='Introduction to Self-Attention', goal='Understand the concept of self-attention and its role in transformer models.', bullets=['Define self-attention and its significance in natural language processing (NLP).', 'Explain how self-attention differs from traditional attention mechanisms.', 'Introduce the basic architecture of a transformer model, highlighting the self-attention component.'], target_words=300, section_type='intro'), Task(id=2, title='The Problem with Sequence Models', goal='Identify the limitations of traditional sequence models that self-attention addresses.', bullets=['Discuss the vanishing gradient problem in RNNs and LSTMs.', 'Explain how fixed-length context windows limit information capture in sequence models.', 'Highlight the inefficiency of processing sequences in a linear manner.'], target_words=350, section_type='core'), Task(id=3, title='Intuition Behind Self-Attention', goal='Grasp the underlying intuition of how self-attention works.', bullets=[\"Illustrate the concept of 'query', 'key', and 'value' vectors in self-attention.\", 'Provide a visual representation of how self-attention computes attention scores.', 'Explain the role of the softmax function in normalizing attention scores.'], target_words=400, section_type='core'), Task(id=4, title='Implementing Self-Attention: A Minimal Working Example', goal='Build a minimal working example of self-attention in Python using NumPy.', bullets=['Provide a code sketch for a self-attention mechanism using NumPy.', 'Include comments in the code to explain each step of the implementation.', 'Demonstrate how to test the implementation with a simple input sequence.'], target_words=450, section_type='examples'), Task(id=5, title='Common Mistakes in Self-Attention Implementation', goal='Recognize and avoid common pitfalls when implementing self-attention.', bullets=['Identify issues with incorrect normalization of attention scores.', 'Discuss the impact of not using appropriate dimensionality for query, key, and value vectors.', 'Explain how failing to handle padding tokens can lead to incorrect attention distributions.'], target_words=350, section_type='common_mistakes'), Task(id=6, title='Performance Considerations of Self-Attention', goal='Evaluate the performance implications of using self-attention in large models.', bullets=['Discuss the computational complexity of self-attention and its impact on training time.', 'Analyze memory usage when scaling self-attention to long sequences.', 'Provide strategies for optimizing self-attention, such as using sparse attention mechanisms.'], target_words=400, section_type='core'), Task(id=7, title='Conclusion and Next Steps', goal='Summarize key takeaways and suggest further learning paths.', bullets=['Recap the importance of self-attention in modern NLP tasks.', 'Suggest resources for deeper understanding, such as research papers and online courses.', 'Encourage experimentation with self-attention in personal projects or open-source contributions.'], target_words=250, section_type='conclusion')]), 'section_results': ['## Introduction to Self-Attention\\n\\nSelf-attention is a mechanism that allows a model to weigh the importance of different words in a sentence relative to each other. In natural language processing (NLP), this is significant because it enables the model to capture contextual relationships between words, regardless of their position in the text. For example, in the sentence \"The cat sat on the mat,\" self-attention helps the model understand that \"cat\" and \"sat\" are closely related, even though they are separated by other words.\\n\\nSelf-attention differs from traditional attention mechanisms, which typically focus on aligning input and output sequences, such as in sequence-to-sequence models. In traditional attention, the model computes attention scores based on the relationship between the encoder\\'s output and the decoder\\'s input. In contrast, self-attention computes attention scores within a single sequence, allowing each word to attend to every other word in the same input sequence. This results in a richer representation of the input, as each word can consider the entire context when forming its representation.\\n\\nThe basic architecture of a transformer model consists of an encoder and a decoder, each made up of multiple layers. The self-attention component is a key part of both the encoder and decoder. In the encoder, self-attention allows the model to create a contextualized representation of the input sequence. Each layer of the encoder computes self-attention using the following steps:\\n\\n1. **Input Representation**: Each word is converted into a vector using embeddings.\\n2. **Query, Key, Value Vectors**: For each word, three vectors are computed: Query (Q), Key (K), and Value (V).\\n3. **Attention Scores**: The attention scores are calculated using the dot product of the Query and Key vectors, followed by a softmax operation to normalize the scores.\\n4. **Weighted Sum**: The Value vectors are weighted by the attention scores to produce the final output for that word.\\n\\nThis architecture allows transformers to efficiently process sequences in parallel, leading to improved performance on various NLP tasks. However, it is important to note that self-attention can be computationally expensive, especially for long sequences, as the complexity is O(n^2) with respect to the sequence length. To mitigate this, techniques like sparse attention or limiting the context window can be employed.', '## The Problem with Sequence Models\\n\\nTraditional sequence models, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs), face several limitations that self-attention mechanisms in Transformer models effectively address.\\n\\n- **Vanishing Gradient Problem**: RNNs and LSTMs are susceptible to the vanishing gradient problem, where gradients diminish exponentially as they propagate back through time. This makes it difficult for these models to learn long-range dependencies. For instance, in a sequence of 1000 tokens, the influence of the first token on the last may become negligible during training, leading to poor performance on tasks requiring long-term context.\\n\\n- **Fixed-Length Context Windows**: Traditional sequence models often utilize fixed-length context windows, which restrict the amount of information they can capture at any given time. For example, if an LSTM is configured to only consider the last 10 tokens, it cannot leverage relevant information from earlier tokens in the sequence. This limitation can lead to suboptimal understanding of context, especially in tasks like language translation or sentiment analysis where distant words may be crucial.\\n\\n- **Inefficiency of Linear Processing**: RNNs process sequences in a linear manner, meaning each token must be processed sequentially. This results in a time complexity of O(n) for a sequence of length n, making it inefficient for long sequences. In contrast, self-attention allows for parallel processing of tokens, significantly reducing computation time and enabling the model to capture relationships between all tokens simultaneously.\\n\\nBy addressing these limitations, self-attention mechanisms enhance the ability of models to understand and generate complex sequences, leading to improved performance across various natural language processing tasks.', '## Intuition Behind Self-Attention\\n\\nSelf-attention is a mechanism that allows a model to weigh the importance of different words in a sequence when encoding a particular word. This is achieved through the use of three vectors: **query**, **key**, and **value**.\\n\\n- **Query**: Represents the word for which we are calculating attention.\\n- **Key**: Represents the words in the sequence that we are comparing against the query.\\n- **Value**: Contains the actual information we want to retrieve based on the attention scores.\\n\\nFor example, consider the sentence \"The cat sat on the mat.\" If we are focusing on the word \"cat\", the query vector will represent \"cat\", while the key vectors will represent all words in the sentence, and the value vectors will also represent all words.\\n\\n### Visual Representation of Attention Scores\\n\\nThe self-attention mechanism computes attention scores by taking the dot product of the query vector with each key vector. This results in a score that indicates how much focus the model should place on each word when processing the query.\\n\\n1. Compute attention scores:\\n   ```python\\n   import numpy as np\\n\\n   query = np.array([1, 0, 0])  # Example query vector\\n   keys = np.array([[1, 0, 0],   # Key vectors\\n                    [0, 1, 0],\\n                    [0, 0, 1]])\\n   scores = np.dot(keys, query)  # Dot product\\n   ```\\n\\n2. The resulting scores might look like this: `[1, 0, 0]`, indicating that the first word (the key for \"The\") is most relevant to the query.\\n\\n### Role of the Softmax Function\\n\\nTo convert these raw scores into probabilities, we apply the **softmax** function. This function normalizes the scores so that they sum to 1, making it easier to interpret them as probabilities.\\n\\n- **Softmax Formula**:\\n  \\\\[\\n  \\\\text{softmax}(x_i) = \\\\frac{e^{x_i}}{\\\\sum_{j} e^{x_j}}\\n  \\\\]\\n\\nUsing the scores from the previous example, applying softmax would yield:\\n```python\\ndef softmax(scores):\\n    exp_scores = np.exp(scores - np.max(scores))  # For numerical stability\\n    return exp_scores / np.sum(exp_scores)\\n\\nattention_weights = softmax(scores)\\n```\\n\\nThis results in a distribution over the words, allowing the model to focus more on relevant words while diminishing the influence of less relevant ones. \\n\\n### Trade-offs and Edge Cases\\n\\nWhile self-attention is powerful, it can be computationally expensive, especially for long sequences, as the complexity is \\\\(O(n^2)\\\\) due to pairwise comparisons. In practice, using techniques like sparse attention or limiting the context window can help mitigate this.\\n\\nEdge cases include handling sequences with varying lengths or padding tokens. It\\'s essential to mask these tokens during the attention calculation to prevent them from influencing the results.', '## Implementing Self-Attention: A Minimal Working Example\\n\\nTo implement a self-attention mechanism in Python using NumPy, we will create a minimal example that captures the essence of self-attention. Below is a code sketch that outlines the key components of the self-attention mechanism.\\n\\n```python\\nimport numpy as np\\n\\ndef softmax(x):\\n    \"\"\"Compute softmax values for each set of scores in x.\"\"\"\\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\\n    return e_x / e_x.sum(axis=-1, keepdims=True)\\n\\ndef self_attention(input_seq):\\n    \"\"\"Compute self-attention for the input sequence.\"\"\"\\n    # Step 1: Create query, key, and value matrices\\n    query = input_seq\\n    key = input_seq\\n    value = input_seq\\n\\n    # Step 2: Calculate the dot products of the query with all keys\\n    scores = np.dot(query, key.T)\\n\\n    # Step 3: Apply softmax to get attention weights\\n    attention_weights = softmax(scores)\\n\\n    # Step 4: Compute the weighted sum of the values\\n    output = np.dot(attention_weights, value)\\n\\n    return output, attention_weights\\n\\n# Test the implementation with a simple input sequence\\ninput_sequence = np.array([[1, 0, 1], \\n                            [0, 1, 0], \\n                            [1, 1, 1]])  # Example input (3 tokens with 3 features each)\\n\\noutput, attention_weights = self_attention(input_sequence)\\n\\nprint(\"Output:\\\\n\", output)\\nprint(\"Attention Weights:\\\\n\", attention_weights)\\n```\\n\\n### Explanation of the Code\\n\\n1. **Softmax Function**: This function normalizes the scores to ensure they sum to 1, which is crucial for interpreting them as probabilities.\\n2. **Self-Attention Function**:\\n   - **Query, Key, Value Matrices**: In this minimal example, we use the same input sequence for queries, keys, and values.\\n   - **Dot Product**: We compute the dot product of the query with all keys to get the attention scores.\\n   - **Softmax Application**: The scores are passed through the softmax function to obtain attention weights.\\n   - **Weighted Sum**: Finally, we compute the output as a weighted sum of the values based on the attention weights.\\n\\n### Testing the Implementation\\n\\nThe provided input sequence consists of three tokens, each represented by a three-dimensional vector. The output and attention weights are printed to verify the correctness of the implementation.\\n\\n### Trade-offs and Edge Cases\\n\\n- **Performance**: This implementation is straightforward but may not be optimized for large sequences. Consider using libraries like TensorFlow or PyTorch for production-level performance.\\n- **Edge Cases**: If the input sequence is empty or contains NaN values, the implementation will fail. Always validate input data before processing.\\n- **Best Practice**: Using separate matrices for queries, keys, and values allows for more flexibility and better performance in complex models.', '## Common Mistakes in Self-Attention Implementation\\n\\nWhen implementing self-attention in Transformer models, several common pitfalls can lead to suboptimal performance or incorrect results. Here are key issues to watch out for:\\n\\n- **Incorrect Normalization of Attention Scores**: \\n  One of the most frequent mistakes is neglecting to apply the softmax function correctly to the attention scores. The attention scores, calculated as the dot product of the query and key vectors, should be scaled by the square root of the dimension of the key vectors to prevent large values that can lead to saturation in the softmax function. \\n\\n  ```python\\n  import numpy as np\\n\\n  def scaled_dot_product_attention(query, key, value):\\n      d_k = query.shape[-1]\\n      scores = np.dot(query, key.T) / np.sqrt(d_k)\\n      attention_weights = softmax(scores)\\n      return np.dot(attention_weights, value)\\n  ```\\n\\n- **Inappropriate Dimensionality for Query, Key, and Value Vectors**: \\n  Ensure that the dimensions of the query, key, and value vectors are consistent. A common mistake is to use different dimensions, which can lead to runtime errors or incorrect attention calculations. Typically, all three should have the same last dimension, which is often set to `d_model` (the model dimension). \\n\\n  - **Checklist**:\\n    - Verify that `query.shape[-1] == key.shape[-1] == value.shape[-1]`.\\n    - Ensure that the dimensionality aligns with the model architecture.\\n\\n- **Failing to Handle Padding Tokens**: \\n  Padding tokens are often used to ensure uniform input lengths. If not handled correctly, they can skew the attention distribution, causing the model to focus on these irrelevant tokens. To mitigate this, apply a mask to the attention scores before the softmax operation, setting the scores of padding tokens to a very negative value (e.g., `-inf`).\\n\\n  ```python\\n  def masked_attention(scores, mask):\\n      scores = np.where(mask, scores, -np.inf)\\n      attention_weights = softmax(scores)\\n      return attention_weights\\n  ```\\n\\nBy addressing these common mistakes, you can enhance the reliability and performance of your self-attention implementation, ensuring that the model learns effectively from the input data.', '## Performance Considerations of Self-Attention\\n\\nSelf-attention mechanisms are a core component of transformer models, but they come with significant performance implications that developers must consider.\\n\\n### Computational Complexity\\n\\nThe computational complexity of self-attention is \\\\(O(n^2 \\\\cdot d)\\\\), where \\\\(n\\\\) is the sequence length and \\\\(d\\\\) is the dimensionality of the input embeddings. This quadratic scaling means that as the sequence length increases, the time required for training can grow rapidly. For example, a sequence of length 512 requires 262,144 operations, while a sequence of length 1024 requires over a million operations. This can lead to longer training times and increased costs, especially when using large datasets or complex models.\\n\\n### Memory Usage\\n\\nMemory usage is another critical factor when scaling self-attention to long sequences. The attention weights matrix, which is \\\\(n \\\\times n\\\\), can consume a significant amount of memory. For instance, with a sequence length of 1024 and embedding size of 768, the attention weights alone can require approximately 6 MB of memory. As sequences grow longer, this can lead to out-of-memory errors on GPUs. To mitigate this, consider using techniques like gradient checkpointing, which trades off some computation for reduced memory usage.\\n\\n### Optimization Strategies\\n\\nTo optimize self-attention, consider the following strategies:\\n\\n- **Sparse Attention Mechanisms**: Implement sparse attention to reduce the number of computations. Techniques like Longformer or Reformer use local attention patterns, which can lower complexity to \\\\(O(n \\\\cdot d)\\\\) or even \\\\(O(\\\\log n \\\\cdot d)\\\\).\\n  \\n- **Low-Rank Approximations**: Use low-rank approximations to reduce the dimensionality of the attention matrix, which can help in both computation and memory usage.\\n\\n- **Chunking**: Process long sequences in smaller chunks, allowing for manageable memory usage while still capturing long-range dependencies.\\n\\nBy applying these strategies, you can significantly improve the performance of self-attention in large models, balancing the trade-offs between speed, memory, and model accuracy.', '## Conclusion and Next Steps\\n\\nSelf-attention is a cornerstone of modern Natural Language Processing (NLP) tasks, enabling models to weigh the significance of different words in a sentence relative to each other. This mechanism allows for better context understanding, leading to improved performance in tasks such as translation, summarization, and sentiment analysis. By capturing long-range dependencies, self-attention enhances the model\\'s ability to generate coherent and contextually relevant outputs.\\n\\nTo deepen your understanding of self-attention, consider exploring the following resources:\\n\\n- **Research Papers**:\\n  - \"Attention is All You Need\" by Vaswani et al. (2017) - The foundational paper introducing the Transformer architecture.\\n  - \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" by Devlin et al. (2018) - A key paper on leveraging self-attention for pre-trained models.\\n\\n- **Online Courses**:\\n  - Coursera\\'s \"Natural Language Processing Specialization\" - Offers a comprehensive overview of NLP techniques, including self-attention.\\n  - Fast.ai\\'s \"Practical Deep Learning for Coders\" - Focuses on practical applications of deep learning, including Transformers.\\n\\nI encourage you to experiment with self-attention in your personal projects or contribute to open-source initiatives. Implementing self-attention from scratch or modifying existing models can provide invaluable hands-on experience. For instance, try building a simple Transformer model using libraries like TensorFlow or PyTorch, and analyze how changes in the self-attention mechanism affect performance.\\n\\nBy actively engaging with these resources and projects, you will solidify your understanding of self-attention and its applications in NLP.'], 'final_blog': '# Understanding Self-Attention in Transformer Models\\n\\n## Introduction to Self-Attention\\n\\nSelf-attention is a mechanism that allows a model to weigh the importance of different words in a sentence relative to each other. In natural language processing (NLP), this is significant because it enables the model to capture contextual relationships between words, regardless of their position in the text. For example, in the sentence \"The cat sat on the mat,\" self-attention helps the model understand that \"cat\" and \"sat\" are closely related, even though they are separated by other words.\\n\\nSelf-attention differs from traditional attention mechanisms, which typically focus on aligning input and output sequences, such as in sequence-to-sequence models. In traditional attention, the model computes attention scores based on the relationship between the encoder\\'s output and the decoder\\'s input. In contrast, self-attention computes attention scores within a single sequence, allowing each word to attend to every other word in the same input sequence. This results in a richer representation of the input, as each word can consider the entire context when forming its representation.\\n\\nThe basic architecture of a transformer model consists of an encoder and a decoder, each made up of multiple layers. The self-attention component is a key part of both the encoder and decoder. In the encoder, self-attention allows the model to create a contextualized representation of the input sequence. Each layer of the encoder computes self-attention using the following steps:\\n\\n1. **Input Representation**: Each word is converted into a vector using embeddings.\\n2. **Query, Key, Value Vectors**: For each word, three vectors are computed: Query (Q), Key (K), and Value (V).\\n3. **Attention Scores**: The attention scores are calculated using the dot product of the Query and Key vectors, followed by a softmax operation to normalize the scores.\\n4. **Weighted Sum**: The Value vectors are weighted by the attention scores to produce the final output for that word.\\n\\nThis architecture allows transformers to efficiently process sequences in parallel, leading to improved performance on various NLP tasks. However, it is important to note that self-attention can be computationally expensive, especially for long sequences, as the complexity is O(n^2) with respect to the sequence length. To mitigate this, techniques like sparse attention or limiting the context window can be employed.\\n\\n## The Problem with Sequence Models\\n\\nTraditional sequence models, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs), face several limitations that self-attention mechanisms in Transformer models effectively address.\\n\\n- **Vanishing Gradient Problem**: RNNs and LSTMs are susceptible to the vanishing gradient problem, where gradients diminish exponentially as they propagate back through time. This makes it difficult for these models to learn long-range dependencies. For instance, in a sequence of 1000 tokens, the influence of the first token on the last may become negligible during training, leading to poor performance on tasks requiring long-term context.\\n\\n- **Fixed-Length Context Windows**: Traditional sequence models often utilize fixed-length context windows, which restrict the amount of information they can capture at any given time. For example, if an LSTM is configured to only consider the last 10 tokens, it cannot leverage relevant information from earlier tokens in the sequence. This limitation can lead to suboptimal understanding of context, especially in tasks like language translation or sentiment analysis where distant words may be crucial.\\n\\n- **Inefficiency of Linear Processing**: RNNs process sequences in a linear manner, meaning each token must be processed sequentially. This results in a time complexity of O(n) for a sequence of length n, making it inefficient for long sequences. In contrast, self-attention allows for parallel processing of tokens, significantly reducing computation time and enabling the model to capture relationships between all tokens simultaneously.\\n\\nBy addressing these limitations, self-attention mechanisms enhance the ability of models to understand and generate complex sequences, leading to improved performance across various natural language processing tasks.\\n\\n## Intuition Behind Self-Attention\\n\\nSelf-attention is a mechanism that allows a model to weigh the importance of different words in a sequence when encoding a particular word. This is achieved through the use of three vectors: **query**, **key**, and **value**.\\n\\n- **Query**: Represents the word for which we are calculating attention.\\n- **Key**: Represents the words in the sequence that we are comparing against the query.\\n- **Value**: Contains the actual information we want to retrieve based on the attention scores.\\n\\nFor example, consider the sentence \"The cat sat on the mat.\" If we are focusing on the word \"cat\", the query vector will represent \"cat\", while the key vectors will represent all words in the sentence, and the value vectors will also represent all words.\\n\\n### Visual Representation of Attention Scores\\n\\nThe self-attention mechanism computes attention scores by taking the dot product of the query vector with each key vector. This results in a score that indicates how much focus the model should place on each word when processing the query.\\n\\n1. Compute attention scores:\\n   ```python\\n   import numpy as np\\n\\n   query = np.array([1, 0, 0])  # Example query vector\\n   keys = np.array([[1, 0, 0],   # Key vectors\\n                    [0, 1, 0],\\n                    [0, 0, 1]])\\n   scores = np.dot(keys, query)  # Dot product\\n   ```\\n\\n2. The resulting scores might look like this: `[1, 0, 0]`, indicating that the first word (the key for \"The\") is most relevant to the query.\\n\\n### Role of the Softmax Function\\n\\nTo convert these raw scores into probabilities, we apply the **softmax** function. This function normalizes the scores so that they sum to 1, making it easier to interpret them as probabilities.\\n\\n- **Softmax Formula**:\\n  \\\\[\\n  \\\\text{softmax}(x_i) = \\\\frac{e^{x_i}}{\\\\sum_{j} e^{x_j}}\\n  \\\\]\\n\\nUsing the scores from the previous example, applying softmax would yield:\\n```python\\ndef softmax(scores):\\n    exp_scores = np.exp(scores - np.max(scores))  # For numerical stability\\n    return exp_scores / np.sum(exp_scores)\\n\\nattention_weights = softmax(scores)\\n```\\n\\nThis results in a distribution over the words, allowing the model to focus more on relevant words while diminishing the influence of less relevant ones. \\n\\n### Trade-offs and Edge Cases\\n\\nWhile self-attention is powerful, it can be computationally expensive, especially for long sequences, as the complexity is \\\\(O(n^2)\\\\) due to pairwise comparisons. In practice, using techniques like sparse attention or limiting the context window can help mitigate this.\\n\\nEdge cases include handling sequences with varying lengths or padding tokens. It\\'s essential to mask these tokens during the attention calculation to prevent them from influencing the results.\\n\\n## Implementing Self-Attention: A Minimal Working Example\\n\\nTo implement a self-attention mechanism in Python using NumPy, we will create a minimal example that captures the essence of self-attention. Below is a code sketch that outlines the key components of the self-attention mechanism.\\n\\n```python\\nimport numpy as np\\n\\ndef softmax(x):\\n    \"\"\"Compute softmax values for each set of scores in x.\"\"\"\\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\\n    return e_x / e_x.sum(axis=-1, keepdims=True)\\n\\ndef self_attention(input_seq):\\n    \"\"\"Compute self-attention for the input sequence.\"\"\"\\n    # Step 1: Create query, key, and value matrices\\n    query = input_seq\\n    key = input_seq\\n    value = input_seq\\n\\n    # Step 2: Calculate the dot products of the query with all keys\\n    scores = np.dot(query, key.T)\\n\\n    # Step 3: Apply softmax to get attention weights\\n    attention_weights = softmax(scores)\\n\\n    # Step 4: Compute the weighted sum of the values\\n    output = np.dot(attention_weights, value)\\n\\n    return output, attention_weights\\n\\n# Test the implementation with a simple input sequence\\ninput_sequence = np.array([[1, 0, 1], \\n                            [0, 1, 0], \\n                            [1, 1, 1]])  # Example input (3 tokens with 3 features each)\\n\\noutput, attention_weights = self_attention(input_sequence)\\n\\nprint(\"Output:\\\\n\", output)\\nprint(\"Attention Weights:\\\\n\", attention_weights)\\n```\\n\\n### Explanation of the Code\\n\\n1. **Softmax Function**: This function normalizes the scores to ensure they sum to 1, which is crucial for interpreting them as probabilities.\\n2. **Self-Attention Function**:\\n   - **Query, Key, Value Matrices**: In this minimal example, we use the same input sequence for queries, keys, and values.\\n   - **Dot Product**: We compute the dot product of the query with all keys to get the attention scores.\\n   - **Softmax Application**: The scores are passed through the softmax function to obtain attention weights.\\n   - **Weighted Sum**: Finally, we compute the output as a weighted sum of the values based on the attention weights.\\n\\n### Testing the Implementation\\n\\nThe provided input sequence consists of three tokens, each represented by a three-dimensional vector. The output and attention weights are printed to verify the correctness of the implementation.\\n\\n### Trade-offs and Edge Cases\\n\\n- **Performance**: This implementation is straightforward but may not be optimized for large sequences. Consider using libraries like TensorFlow or PyTorch for production-level performance.\\n- **Edge Cases**: If the input sequence is empty or contains NaN values, the implementation will fail. Always validate input data before processing.\\n- **Best Practice**: Using separate matrices for queries, keys, and values allows for more flexibility and better performance in complex models.\\n\\n## Common Mistakes in Self-Attention Implementation\\n\\nWhen implementing self-attention in Transformer models, several common pitfalls can lead to suboptimal performance or incorrect results. Here are key issues to watch out for:\\n\\n- **Incorrect Normalization of Attention Scores**: \\n  One of the most frequent mistakes is neglecting to apply the softmax function correctly to the attention scores. The attention scores, calculated as the dot product of the query and key vectors, should be scaled by the square root of the dimension of the key vectors to prevent large values that can lead to saturation in the softmax function. \\n\\n  ```python\\n  import numpy as np\\n\\n  def scaled_dot_product_attention(query, key, value):\\n      d_k = query.shape[-1]\\n      scores = np.dot(query, key.T) / np.sqrt(d_k)\\n      attention_weights = softmax(scores)\\n      return np.dot(attention_weights, value)\\n  ```\\n\\n- **Inappropriate Dimensionality for Query, Key, and Value Vectors**: \\n  Ensure that the dimensions of the query, key, and value vectors are consistent. A common mistake is to use different dimensions, which can lead to runtime errors or incorrect attention calculations. Typically, all three should have the same last dimension, which is often set to `d_model` (the model dimension). \\n\\n  - **Checklist**:\\n    - Verify that `query.shape[-1] == key.shape[-1] == value.shape[-1]`.\\n    - Ensure that the dimensionality aligns with the model architecture.\\n\\n- **Failing to Handle Padding Tokens**: \\n  Padding tokens are often used to ensure uniform input lengths. If not handled correctly, they can skew the attention distribution, causing the model to focus on these irrelevant tokens. To mitigate this, apply a mask to the attention scores before the softmax operation, setting the scores of padding tokens to a very negative value (e.g., `-inf`).\\n\\n  ```python\\n  def masked_attention(scores, mask):\\n      scores = np.where(mask, scores, -np.inf)\\n      attention_weights = softmax(scores)\\n      return attention_weights\\n  ```\\n\\nBy addressing these common mistakes, you can enhance the reliability and performance of your self-attention implementation, ensuring that the model learns effectively from the input data.\\n\\n## Performance Considerations of Self-Attention\\n\\nSelf-attention mechanisms are a core component of transformer models, but they come with significant performance implications that developers must consider.\\n\\n### Computational Complexity\\n\\nThe computational complexity of self-attention is \\\\(O(n^2 \\\\cdot d)\\\\), where \\\\(n\\\\) is the sequence length and \\\\(d\\\\) is the dimensionality of the input embeddings. This quadratic scaling means that as the sequence length increases, the time required for training can grow rapidly. For example, a sequence of length 512 requires 262,144 operations, while a sequence of length 1024 requires over a million operations. This can lead to longer training times and increased costs, especially when using large datasets or complex models.\\n\\n### Memory Usage\\n\\nMemory usage is another critical factor when scaling self-attention to long sequences. The attention weights matrix, which is \\\\(n \\\\times n\\\\), can consume a significant amount of memory. For instance, with a sequence length of 1024 and embedding size of 768, the attention weights alone can require approximately 6 MB of memory. As sequences grow longer, this can lead to out-of-memory errors on GPUs. To mitigate this, consider using techniques like gradient checkpointing, which trades off some computation for reduced memory usage.\\n\\n### Optimization Strategies\\n\\nTo optimize self-attention, consider the following strategies:\\n\\n- **Sparse Attention Mechanisms**: Implement sparse attention to reduce the number of computations. Techniques like Longformer or Reformer use local attention patterns, which can lower complexity to \\\\(O(n \\\\cdot d)\\\\) or even \\\\(O(\\\\log n \\\\cdot d)\\\\).\\n  \\n- **Low-Rank Approximations**: Use low-rank approximations to reduce the dimensionality of the attention matrix, which can help in both computation and memory usage.\\n\\n- **Chunking**: Process long sequences in smaller chunks, allowing for manageable memory usage while still capturing long-range dependencies.\\n\\nBy applying these strategies, you can significantly improve the performance of self-attention in large models, balancing the trade-offs between speed, memory, and model accuracy.\\n\\n## Conclusion and Next Steps\\n\\nSelf-attention is a cornerstone of modern Natural Language Processing (NLP) tasks, enabling models to weigh the significance of different words in a sentence relative to each other. This mechanism allows for better context understanding, leading to improved performance in tasks such as translation, summarization, and sentiment analysis. By capturing long-range dependencies, self-attention enhances the model\\'s ability to generate coherent and contextually relevant outputs.\\n\\nTo deepen your understanding of self-attention, consider exploring the following resources:\\n\\n- **Research Papers**:\\n  - \"Attention is All You Need\" by Vaswani et al. (2017) - The foundational paper introducing the Transformer architecture.\\n  - \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" by Devlin et al. (2018) - A key paper on leveraging self-attention for pre-trained models.\\n\\n- **Online Courses**:\\n  - Coursera\\'s \"Natural Language Processing Specialization\" - Offers a comprehensive overview of NLP techniques, including self-attention.\\n  - Fast.ai\\'s \"Practical Deep Learning for Coders\" - Focuses on practical applications of deep learning, including Transformers.\\n\\nI encourage you to experiment with self-attention in your personal projects or contribute to open-source initiatives. Implementing self-attention from scratch or modifying existing models can provide invaluable hands-on experience. For instance, try building a simple Transformer model using libraries like TensorFlow or PyTorch, and analyze how changes in the self-attention mechanism affect performance.\\n\\nBy actively engaging with these resources and projects, you will solidify your understanding of self-attention and its applications in NLP.'}\n"
          ]
        }
      ],
      "source": [
        "input_state = {\"topic\":\"Self-Attention in Transformer Models\"}\n",
        "\n",
        "output_state = app.invoke(input_state)\n",
        "print(output_state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "2a73c42b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task Title: Introduction to Self-Attention\n",
            "Task Goal: Understand the concept of self-attention and its role in transformer models.\n",
            "Task Bullets: ['Define self-attention and its significance in natural language processing (NLP).', 'Explain how self-attention differs from traditional attention mechanisms.', 'Introduce the basic architecture of a transformer model, highlighting the self-attention component.']\n",
            "\n",
            "\n",
            "Task Title: The Problem with Sequence Models\n",
            "Task Goal: Identify the limitations of traditional sequence models that self-attention addresses.\n",
            "Task Bullets: ['Discuss the vanishing gradient problem in RNNs and LSTMs.', 'Explain how fixed-length context windows limit information capture in sequence models.', 'Highlight the inefficiency of processing sequences in a linear manner.']\n",
            "\n",
            "\n",
            "Task Title: Intuition Behind Self-Attention\n",
            "Task Goal: Grasp the underlying intuition of how self-attention works.\n",
            "Task Bullets: [\"Illustrate the concept of 'query', 'key', and 'value' vectors in self-attention.\", 'Provide a visual representation of how self-attention computes attention scores.', 'Explain the role of the softmax function in normalizing attention scores.']\n",
            "\n",
            "\n",
            "Task Title: Implementing Self-Attention: A Minimal Working Example\n",
            "Task Goal: Build a minimal working example of self-attention in Python using NumPy.\n",
            "Task Bullets: ['Provide a code sketch for a self-attention mechanism using NumPy.', 'Include comments in the code to explain each step of the implementation.', 'Demonstrate how to test the implementation with a simple input sequence.']\n",
            "\n",
            "\n",
            "Task Title: Common Mistakes in Self-Attention Implementation\n",
            "Task Goal: Recognize and avoid common pitfalls when implementing self-attention.\n",
            "Task Bullets: ['Identify issues with incorrect normalization of attention scores.', 'Discuss the impact of not using appropriate dimensionality for query, key, and value vectors.', 'Explain how failing to handle padding tokens can lead to incorrect attention distributions.']\n",
            "\n",
            "\n",
            "Task Title: Performance Considerations of Self-Attention\n",
            "Task Goal: Evaluate the performance implications of using self-attention in large models.\n",
            "Task Bullets: ['Discuss the computational complexity of self-attention and its impact on training time.', 'Analyze memory usage when scaling self-attention to long sequences.', 'Provide strategies for optimizing self-attention, such as using sparse attention mechanisms.']\n",
            "\n",
            "\n",
            "Task Title: Conclusion and Next Steps\n",
            "Task Goal: Summarize key takeaways and suggest further learning paths.\n",
            "Task Bullets: ['Recap the importance of self-attention in modern NLP tasks.', 'Suggest resources for deeper understanding, such as research papers and online courses.', 'Encourage experimentation with self-attention in personal projects or open-source contributions.']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for task in output_state['plan'].tasks:\n",
        "    print(f\"Task Title: {task.title}\")\n",
        "    print(f\"Task Goal: {task.goal}\")\n",
        "    print(f\"Task Bullets: {task.bullets}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a44a0b8",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchainvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
