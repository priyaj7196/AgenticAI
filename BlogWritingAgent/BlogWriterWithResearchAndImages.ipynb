{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "476ded0b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/priya/LangChain/langchainvenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load .env from project root (works when notebook runs from LangGraph/BlogWriter)\n",
        "load_dotenv(Path.cwd() / \".env\") or load_dotenv(Path.cwd().parent.parent / \".env\")\n",
        "\n",
        "import operator\n",
        "import os\n",
        "import re\n",
        "from datetime import date, timedelta\n",
        "from typing import TypedDict, List, Optional, Literal, Annotated\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.types import Send\n",
        "\n",
        "import httpx\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ecb4e276",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 1) Schemas\n",
        "# -----------------------------\n",
        "class Task(BaseModel):\n",
        "    id: int\n",
        "    title: str\n",
        "\n",
        "    goal: str = Field(\n",
        "        ...,\n",
        "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
        "    )\n",
        "    bullets: List[str] = Field(\n",
        "        ...,\n",
        "        min_length=3,\n",
        "        max_length=6,\n",
        "        description=\"3–6 concrete, non-overlapping subpoints to cover in this section.\",\n",
        "    )\n",
        "    target_words: int = Field(..., description=\"Target word count for this section (120–550).\")\n",
        "\n",
        "    tags: List[str] = Field(default_factory=list)\n",
        "    requires_research: bool = False\n",
        "    requires_citations: bool = False\n",
        "    requires_code: bool = False\n",
        "\n",
        "\n",
        "class Plan(BaseModel):\n",
        "    blog_title: str\n",
        "    audience: str\n",
        "    tone: str\n",
        "    blog_kind: Literal[\"explainer\", \"tutorial\", \"news_roundup\", \"comparison\", \"system_design\"] = \"explainer\"\n",
        "    constraints: List[str] = Field(default_factory=list)\n",
        "    tasks: List[Task]\n",
        "\n",
        "\n",
        "class EvidenceItem(BaseModel):\n",
        "    title: str\n",
        "    url: str\n",
        "    published_at: Optional[str] = None  # keep if Tavily provides; DO NOT rely on it\n",
        "    snippet: Optional[str] = None\n",
        "    source: Optional[str] = None\n",
        "\n",
        "\n",
        "class RouterDecision(BaseModel):\n",
        "    needs_research: bool\n",
        "    mode: Literal[\"closed_book\", \"hybrid\", \"open_book\"]\n",
        "    queries: List[str] = Field(default_factory=list)\n",
        "\n",
        "\n",
        "class EvidencePack(BaseModel):\n",
        "    evidence: List[EvidenceItem] = Field(default_factory=list)\n",
        "\n",
        "\n",
        "class ImageSpec(BaseModel):\n",
        "    placeholder: str = Field(..., description=\"e.g. [[IMAGE_1]]\")\n",
        "    filename: str = Field(..., description=\"Save under images/, e.g. qkv_flow.png\")\n",
        "    alt: str\n",
        "    caption: str\n",
        "    prompt: str = Field(..., description=\"Prompt to send to the image model.\")\n",
        "    size: Literal[\"1024x1024\", \"1024x1536\", \"1536x1024\"] = \"1024x1024\"\n",
        "    quality: Literal[\"low\", \"medium\", \"high\"] = \"medium\"\n",
        "\n",
        "\n",
        "class GlobalImagePlan(BaseModel):\n",
        "    md_with_placeholders: str\n",
        "    images: List[ImageSpec] = Field(default_factory=list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6d9f8957",
      "metadata": {},
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    topic: str\n",
        "\n",
        "    # routing / research\n",
        "    mode: str\n",
        "    needs_research: bool\n",
        "    queries: List[str]\n",
        "    evidence: List[EvidenceItem]\n",
        "    plan: Optional[Plan]\n",
        "\n",
        "    # workers\n",
        "    sections: Annotated[List[tuple[int, str]], operator.add]  # (task_id, section_md)\n",
        "\n",
        "    # reducer/image\n",
        "    merged_md: str\n",
        "    md_with_placeholders: str\n",
        "    image_specs: List[dict]\n",
        "\n",
        "    final: str\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2) LLM\n",
        "# -----------------------------\n",
        "# Pass http_client to avoid openai+httpx 0.28 \"proxies\" compatibility error\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", http_client=httpx.Client())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "da1c0162",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 3) Router (decide upfront)\n",
        "# -----------------------------\n",
        "ROUTER_SYSTEM = \"\"\"You are a routing module for a technical blog planner.\n",
        "\n",
        "Decide whether web research is needed BEFORE planning.\n",
        "\n",
        "Modes:\n",
        "- closed_book (needs_research=false):\n",
        "  Evergreen topics where correctness does not depend on recent facts (concepts, fundamentals).\n",
        "- hybrid (needs_research=true):\n",
        "  Mostly evergreen but needs up-to-date examples/tools/models to be useful.\n",
        "- open_book (needs_research=true):\n",
        "  Mostly volatile: weekly roundups, \"this week\", \"latest\", rankings, pricing, policy/regulation.\n",
        "\n",
        "If needs_research=true:\n",
        "- Output 3–10 high-signal queries.\n",
        "- Queries should be scoped and specific (avoid generic queries like just \"AI\" or \"LLM\").\n",
        "- If user asked for \"last week/this week/latest\", reflect that constraint IN THE QUERIES.\n",
        "\"\"\"\n",
        "\n",
        "def router_node(state: State) -> dict:\n",
        "    \n",
        "    topic = state[\"topic\"]\n",
        "    decider = llm.with_structured_output(RouterDecision)\n",
        "    decision = decider.invoke(\n",
        "        [\n",
        "            SystemMessage(content=ROUTER_SYSTEM),\n",
        "            HumanMessage(content=f\"Topic: {topic}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"needs_research\": decision.needs_research,\n",
        "        \"mode\": decision.mode,\n",
        "        \"queries\": decision.queries,\n",
        "    }\n",
        "\n",
        "def route_next(state: State) -> str:\n",
        "    return \"research\" if state[\"needs_research\"] else \"orchestrator\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e09e9229",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 4) Research (Tavily) \n",
        "# -----------------------------\n",
        "def _tavily_search(query: str, max_results: int = 5) -> List[dict]:\n",
        "    \n",
        "    tool = TavilySearchResults(max_results=max_results)\n",
        "    results = tool.invoke({\"query\": query})\n",
        "\n",
        "    normalized: List[dict] = []\n",
        "    for r in results or []:\n",
        "        normalized.append(\n",
        "            {\n",
        "                \"title\": r.get(\"title\") or \"\",\n",
        "                \"url\": r.get(\"url\") or \"\",\n",
        "                \"snippet\": r.get(\"content\") or r.get(\"snippet\") or \"\",\n",
        "                \"published_at\": r.get(\"published_date\") or r.get(\"published_at\"),\n",
        "                \"source\": r.get(\"source\"),\n",
        "            }\n",
        "        )\n",
        "    return normalized\n",
        "\n",
        "\n",
        "RESEARCH_SYSTEM = \"\"\"You are a research synthesizer for technical writing.\n",
        "\n",
        "Given raw web search results, produce a deduplicated list of EvidenceItem objects.\n",
        "\n",
        "Rules:\n",
        "- Only include items with a non-empty url.\n",
        "- Prefer relevant + authoritative sources (company blogs, docs, reputable outlets).\n",
        "- If a published date is explicitly present in the result payload, keep it as YYYY-MM-DD.\n",
        "  If missing or unclear, set published_at=null. Do NOT guess.\n",
        "- Keep snippets short.\n",
        "- Deduplicate by URL.\n",
        "\"\"\"\n",
        "\n",
        "def research_node(state: State) -> dict:\n",
        "\n",
        "    # take the first 10 queries from state\n",
        "    queries = (state.get(\"queries\", []) or [])\n",
        "    max_results = 6\n",
        "\n",
        "    raw_results: List[dict] = []\n",
        "\n",
        "    for q in queries:\n",
        "        raw_results.extend(_tavily_search(q, max_results=max_results))\n",
        "\n",
        "    if not raw_results:\n",
        "        return {\"evidence\": []}\n",
        "\n",
        "    extractor = llm.with_structured_output(EvidencePack)\n",
        "    pack = extractor.invoke(\n",
        "        [\n",
        "            SystemMessage(content=RESEARCH_SYSTEM),\n",
        "            HumanMessage(content=f\"Raw results:\\n{raw_results}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Deduplicate by URL\n",
        "    dedup = {}\n",
        "    for e in pack.evidence:\n",
        "        if e.url:\n",
        "            dedup[e.url] = e\n",
        "\n",
        "    return {\"evidence\": list(dedup.values())}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1e6c91ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 5) Orchestrator (Plan)\n",
        "# -----------------------------\n",
        "ORCH_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
        "Your job is to produce a highly actionable outline for a technical blog post.\n",
        "\n",
        "Hard requirements:\n",
        "- Create 5–9 sections (tasks) suitable for the topic and audience.\n",
        "- Each task must include:\n",
        "  1) goal (1 sentence)\n",
        "  2) 3–6 bullets that are concrete, specific, and non-overlapping\n",
        "  3) target word count (120–550)\n",
        "\n",
        "Quality bar:\n",
        "- Assume the reader is a developer; use correct terminology.\n",
        "- Bullets must be actionable: build/compare/measure/verify/debug.\n",
        "- Ensure the overall plan includes at least 2 of these somewhere:\n",
        "  * minimal code sketch / MWE (set requires_code=True for that section)\n",
        "  * edge cases / failure modes\n",
        "  * performance/cost considerations\n",
        "  * security/privacy considerations (if relevant)\n",
        "  * debugging/observability tips\n",
        "\n",
        "Grounding rules:\n",
        "- Mode closed_book: keep it evergreen; do not depend on evidence.\n",
        "- Mode hybrid:\n",
        "  - Use evidence for up-to-date examples (models/tools/releases) in bullets.\n",
        "  - Mark sections using fresh info as requires_research=True and requires_citations=True.\n",
        "- Mode open_book:\n",
        "  - Set blog_kind = \"news_roundup\".\n",
        "  - Every section is about summarizing events + implications.\n",
        "  - DO NOT include tutorial/how-to sections unless user explicitly asked for that.\n",
        "  - If evidence is empty or insufficient, create a plan that transparently says \"insufficient sources\"\n",
        "    and includes only what can be supported.\n",
        "\n",
        "Output must strictly match the Plan schema.\n",
        "\"\"\"\n",
        "\n",
        "def orchestrator_node(state: State) -> dict:\n",
        "    planner = llm.with_structured_output(Plan)\n",
        "\n",
        "    evidence = state.get(\"evidence\", [])\n",
        "    mode = state.get(\"mode\", \"closed_book\")\n",
        "\n",
        "    plan = planner.invoke(\n",
        "        [\n",
        "            SystemMessage(content=ORCH_SYSTEM),\n",
        "            HumanMessage(\n",
        "                content=(\n",
        "                    f\"Topic: {state['topic']}\\n\"\n",
        "                    f\"Mode: {mode}\\n\\n\"\n",
        "                    f\"Evidence (ONLY use for fresh claims; may be empty):\\n\"\n",
        "                    f\"{[e.model_dump() for e in evidence][:16]}\"\n",
        "                )\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return {\"plan\": plan}\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Fanout\n",
        "# -----------------------------\n",
        "def fanout(state: State):\n",
        "    return [\n",
        "        Send(\n",
        "            \"worker\",\n",
        "            {\n",
        "                \"task\": task.model_dump(),\n",
        "                \"topic\": state[\"topic\"],\n",
        "                \"mode\": state[\"mode\"],\n",
        "                \"plan\": state[\"plan\"].model_dump(),\n",
        "                \"evidence\": [e.model_dump() for e in state.get(\"evidence\", [])],\n",
        "            },\n",
        "        )\n",
        "        for task in state[\"plan\"].tasks\n",
        "    ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f634ce01",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 7) Worker (write one section)\n",
        "# -----------------------------\n",
        "WORKER_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
        "Write ONE section of a technical blog post in Markdown.\n",
        "\n",
        "Hard constraints:\n",
        "- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\n",
        "- Stay close to Target words (±15%).\n",
        "- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\n",
        "- Start with a '## <Section Title>' heading.\n",
        "\n",
        "Scope guard:\n",
        "- If blog_kind == \"news_roundup\": do NOT turn this into a tutorial/how-to guide.\n",
        "  Do NOT teach web scraping, RSS, automation, or \"how to fetch news\" unless bullets explicitly ask for it.\n",
        "  Focus on summarizing events and implications.\n",
        "\n",
        "Grounding policy:\n",
        "- If mode == open_book:\n",
        "  - Do NOT introduce any specific event/company/model/funding/policy claim unless it is supported by provided Evidence URLs.\n",
        "  - For each event claim, attach a source as a Markdown link: ([Source](URL)).\n",
        "  - Only use URLs provided in Evidence. If not supported, write: \"Not found in provided sources.\"\n",
        "- If requires_citations == true:\n",
        "  - For outside-world claims, cite Evidence URLs the same way.\n",
        "- Evergreen reasoning is OK without citations unless requires_citations is true.\n",
        "\n",
        "Code:\n",
        "- If requires_code == true, include at least one minimal, correct code snippet relevant to the bullets.\n",
        "\n",
        "Style:\n",
        "- Short paragraphs, bullets where helpful, code fences for code.\n",
        "- Avoid fluff/marketing. Be precise and implementation-oriented.\n",
        "\"\"\"\n",
        "\n",
        "def worker_node(payload: dict) -> dict:\n",
        "    \n",
        "    task = Task(**payload[\"task\"])\n",
        "    plan = Plan(**payload[\"plan\"])\n",
        "    evidence = [EvidenceItem(**e) for e in payload.get(\"evidence\", [])]\n",
        "    topic = payload[\"topic\"]\n",
        "    mode = payload.get(\"mode\", \"closed_book\")\n",
        "\n",
        "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
        "\n",
        "    evidence_text = \"\"\n",
        "    if evidence:\n",
        "        evidence_text = \"\\n\".join(\n",
        "            f\"- {e.title} | {e.url} | {e.published_at or 'date:unknown'}\".strip()\n",
        "            for e in evidence[:20]\n",
        "        )\n",
        "\n",
        "    section_md = llm.invoke(\n",
        "        [\n",
        "            SystemMessage(content=WORKER_SYSTEM),\n",
        "            HumanMessage(\n",
        "                content=(\n",
        "                    f\"Blog title: {plan.blog_title}\\n\"\n",
        "                    f\"Audience: {plan.audience}\\n\"\n",
        "                    f\"Tone: {plan.tone}\\n\"\n",
        "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
        "                    f\"Constraints: {plan.constraints}\\n\"\n",
        "                    f\"Topic: {topic}\\n\"\n",
        "                    f\"Mode: {mode}\\n\\n\"\n",
        "                    f\"Section title: {task.title}\\n\"\n",
        "                    f\"Goal: {task.goal}\\n\"\n",
        "                    f\"Target words: {task.target_words}\\n\"\n",
        "                    f\"Tags: {task.tags}\\n\"\n",
        "                    f\"requires_research: {task.requires_research}\\n\"\n",
        "                    f\"requires_citations: {task.requires_citations}\\n\"\n",
        "                    f\"requires_code: {task.requires_code}\\n\"\n",
        "                    f\"Bullets:{bullets_text}\\n\\n\"\n",
        "                    f\"Evidence (ONLY use these URLs when citing):\\n{evidence_text}\\n\"\n",
        "                )\n",
        "            ),\n",
        "        ]\n",
        "    ).content.strip()\n",
        "\n",
        "    return {\"sections\": [(task.id, section_md)]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ea4856b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 8) ReducerWithImages (subgraph)\n",
        "#    merge_content -> decide_images -> generate_and_place_images\n",
        "# ============================================================\n",
        "def merge_content(state: State) -> dict:\n",
        "\n",
        "    plan = state[\"plan\"]\n",
        "\n",
        "    ordered_sections = [md for _, md in sorted(state[\"sections\"], key=lambda x: x[0])]\n",
        "    body = \"\\n\\n\".join(ordered_sections).strip()\n",
        "    merged_md = f\"# {plan.blog_title}\\n\\n{body}\\n\"\n",
        "    return {\"merged_md\": merged_md}\n",
        "\n",
        "\n",
        "DECIDE_IMAGES_SYSTEM = \"\"\"You are an expert technical editor.\n",
        "Decide if images/diagrams are needed for THIS blog.\n",
        "\n",
        "Rules:\n",
        "- Max 3 images total.\n",
        "- Each image must materially improve understanding (diagram/flow/table-like visual).\n",
        "- Insert placeholders exactly: [[IMAGE_1]], [[IMAGE_2]], [[IMAGE_3]].\n",
        "- If no images needed: md_with_placeholders must equal input and images=[].\n",
        "- Avoid decorative images; prefer technical diagrams with short labels.\n",
        "Return strictly GlobalImagePlan.\n",
        "\"\"\"\n",
        "\n",
        "def decide_images(state: State) -> dict:\n",
        "    \n",
        "    planner = llm.with_structured_output(GlobalImagePlan)\n",
        "    merged_md = state[\"merged_md\"]\n",
        "    plan = state[\"plan\"]\n",
        "    assert plan is not None\n",
        "\n",
        "    image_plan = planner.invoke(\n",
        "        [\n",
        "            SystemMessage(content=DECIDE_IMAGES_SYSTEM),\n",
        "            HumanMessage(\n",
        "                content=(\n",
        "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
        "                    f\"Topic: {state['topic']}\\n\\n\"\n",
        "                    \"Insert placeholders + propose image prompts.\\n\\n\"\n",
        "                    f\"{merged_md}\"\n",
        "                )\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"md_with_placeholders\": image_plan.md_with_placeholders,\n",
        "        \"image_specs\": [img.model_dump() for img in image_plan.images],\n",
        "    }\n",
        "\n",
        "\n",
        "def _gemini_generate_image_bytes(prompt: str) -> bytes:\n",
        "    \"\"\"\n",
        "    Returns raw image bytes generated by Gemini.\n",
        "    Requires: pip install google-genai\n",
        "    Env var: GOOGLE_API_KEY\n",
        "    \"\"\"\n",
        "    from google import genai\n",
        "    from google.genai import types\n",
        "\n",
        "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"GOOGLE_API_KEY is not set.\")\n",
        "\n",
        "    client = genai.Client(api_key=api_key)\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash-image\",\n",
        "        contents=prompt,\n",
        "        config=types.GenerateContentConfig(\n",
        "            response_modalities=[\"IMAGE\"],\n",
        "            safety_settings=[\n",
        "                types.SafetySetting(\n",
        "                    category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "                    threshold=\"BLOCK_ONLY_HIGH\",\n",
        "                )\n",
        "            ],\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Depending on SDK version, parts may hang off resp.candidates[0].content.parts\n",
        "    parts = getattr(resp, \"parts\", None)\n",
        "    if not parts and getattr(resp, \"candidates\", None):\n",
        "        try:\n",
        "            parts = resp.candidates[0].content.parts\n",
        "        except Exception:\n",
        "            parts = None\n",
        "\n",
        "    if not parts:\n",
        "        raise RuntimeError(\"No image content returned (safety/quota/SDK change).\")\n",
        "\n",
        "    for part in parts:\n",
        "        inline = getattr(part, \"inline_data\", None)\n",
        "        if inline and getattr(inline, \"data\", None):\n",
        "            return inline.data\n",
        "\n",
        "    raise RuntimeError(\"No inline image bytes found in response.\")\n",
        "\n",
        "\n",
        "def generate_and_place_images(state: State) -> dict:\n",
        "\n",
        "    plan = state[\"plan\"]\n",
        "    assert plan is not None\n",
        "\n",
        "    md = state.get(\"md_with_placeholders\") or state[\"merged_md\"]\n",
        "    image_specs = state.get(\"image_specs\", []) or []\n",
        "\n",
        "    # If no images requested, just write merged markdown\n",
        "    if not image_specs:\n",
        "        filename = f\"{plan.blog_title}.md\"\n",
        "        Path(filename).write_text(md, encoding=\"utf-8\")\n",
        "        return {\"final\": md}\n",
        "\n",
        "    images_dir = Path(\"images\")\n",
        "    images_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    for spec in image_specs:\n",
        "        placeholder = spec[\"placeholder\"]\n",
        "        filename = spec[\"filename\"]\n",
        "        out_path = images_dir / filename\n",
        "\n",
        "        # generate only if needed\n",
        "        if not out_path.exists():\n",
        "            try:\n",
        "                img_bytes = _gemini_generate_image_bytes(spec[\"prompt\"])\n",
        "                out_path.write_bytes(img_bytes)\n",
        "            except Exception as e:\n",
        "                # graceful fallback: keep doc usable\n",
        "                prompt_block = (\n",
        "                    f\"> **[IMAGE GENERATION FAILED]** {spec.get('caption','')}\\n>\\n\"\n",
        "                    f\"> **Alt:** {spec.get('alt','')}\\n>\\n\"\n",
        "                    f\"> **Prompt:** {spec.get('prompt','')}\\n>\\n\"\n",
        "                    f\"> **Error:** {e}\\n\"\n",
        "                )\n",
        "                md = md.replace(placeholder, prompt_block)\n",
        "                continue\n",
        "\n",
        "        img_md = f\"![{spec['alt']}](images/{filename})\\n*{spec['caption']}*\"\n",
        "        md = md.replace(placeholder, img_md)\n",
        "\n",
        "    filename = f\"{plan.blog_title}.md\"\n",
        "    Path(filename).write_text(md, encoding=\"utf-8\")\n",
        "    return {\"final\": md}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cebc44ef",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x1189f70e0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# build reducer subgraph\n",
        "reducer_graph = StateGraph(State)\n",
        "reducer_graph.add_node(\"merge_content\", merge_content)\n",
        "reducer_graph.add_node(\"decide_images\", decide_images)\n",
        "reducer_graph.add_node(\"generate_and_place_images\", generate_and_place_images)\n",
        "reducer_graph.add_edge(START, \"merge_content\")\n",
        "reducer_graph.add_edge(\"merge_content\", \"decide_images\")\n",
        "reducer_graph.add_edge(\"decide_images\", \"generate_and_place_images\")\n",
        "reducer_graph.add_edge(\"generate_and_place_images\", END)\n",
        "reducer_subgraph = reducer_graph.compile()\n",
        "\n",
        "reducer_subgraph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "45b41ece",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x118f05450>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# 9) Build main graph\n",
        "# -----------------------------\n",
        "g = StateGraph(State)\n",
        "g.add_node(\"router\", router_node)\n",
        "g.add_node(\"research\", research_node)\n",
        "g.add_node(\"orchestrator\", orchestrator_node)\n",
        "g.add_node(\"worker\", worker_node)\n",
        "g.add_node(\"reducer\", reducer_subgraph)\n",
        "\n",
        "g.add_edge(START, \"router\")\n",
        "g.add_conditional_edges(\"router\", route_next, {\"research\": \"research\", \"orchestrator\": \"orchestrator\"})\n",
        "g.add_edge(\"research\", \"orchestrator\")\n",
        "\n",
        "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
        "g.add_edge(\"worker\", \"reducer\")\n",
        "g.add_edge(\"reducer\", END)\n",
        "\n",
        "app = g.compile()\n",
        "app\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f2f5a07e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 10) Runner\n",
        "# -----------------------------\n",
        "def run(topic: str, as_of: Optional[str] = None):\n",
        "    if as_of is None:\n",
        "        as_of = date.today().isoformat()\n",
        "\n",
        "    out = app.invoke(\n",
        "        {\n",
        "            \"topic\": topic,\n",
        "            \"mode\": \"\",\n",
        "            \"needs_research\": False,\n",
        "            \"queries\": [],\n",
        "            \"evidence\": [],\n",
        "            \"plan\": None,\n",
        "            \"as_of\": as_of,\n",
        "            \"recency_days\": 7,\n",
        "            \"sections\": [],\n",
        "            \"merged_md\": \"\",\n",
        "            \"md_with_placeholders\": \"\",\n",
        "            \"image_specs\": [],\n",
        "            \"final\": \"\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5c066987",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'topic': 'Positional Encoding in Transformer Architecture',\n",
              " 'mode': 'closed_book',\n",
              " 'needs_research': False,\n",
              " 'queries': [],\n",
              " 'evidence': [],\n",
              " 'plan': Plan(blog_title='Positional Encoding in Transformer Architecture', audience='Developers with foundational knowledge of machine learning and deep learning architectures', tone='Technical and informative', blog_kind='explainer', constraints=[], tasks=[Task(id=1, title='Introduction to the Need for Positional Encoding in Transformers', goal='Understand why positional encoding is essential in transformer models and the problem it solves.', bullets=['Explain the transformer’s reliance on self-attention and how it lacks inherent sequence order awareness', 'Describe why sequence order matters in natural language and other sequential data', 'Contrast transformers with recurrent and convolutional models in terms of sequence order handling'], target_words=200, tags=[], requires_research=False, requires_citations=False, requires_code=False), Task(id=2, title='Fundamentals of Positional Encoding', goal='Learn the basic principles and objectives behind positional encoding techniques used in transformers.', bullets=['Define positional encoding as a method to inject sequence order information into input embeddings', 'Outline the requirements: fixed or learnable, differentiable, and compatible with input embeddings', 'Discuss typical dimensions and how positional encodings align with embedding vectors'], target_words=250, tags=[], requires_research=False, requires_citations=False, requires_code=False), Task(id=3, title='Sinusoidal Positional Encoding Explained', goal='Understand the original sinusoidal positional encoding method introduced in the Transformer paper.', bullets=['Present the sinusoidal functions used to generate position encodings for each dimension', 'Explain the mathematical formulation and why it enables the model to generalize to longer sequences', 'Discuss advantages such as fixed encoding, smoothness, and interpretability'], target_words=300, tags=[], requires_research=False, requires_citations=False, requires_code=True), Task(id=4, title='Learnable Positional Embeddings', goal='Explore the alternative approach of learnable positional embeddings and how they differ from fixed encodings.', bullets=['Describe how positional embeddings are parameter vectors learned during training', 'Compare learnable embeddings with sinusoidal encodings in terms of flexibility and generalization', 'Discuss implementation considerations and typical use cases in modern transformer architectures'], target_words=280, tags=[], requires_research=False, requires_citations=False, requires_code=False), Task(id=5, title='Edge Cases and Limitations of Positional Encoding Methods', goal='Identify common challenges and failure modes related to positional encodings in transformers.', bullets=['Examine issues with fixed length in learnable embeddings and potential out-of-vocabulary positions', 'Discuss challenges in generalizing to longer sequences than seen during training', 'Explore how noise or corrupted positional information can affect model performance'], target_words=300, tags=[], requires_research=False, requires_citations=False, requires_code=False), Task(id=6, title='Performance and Computational Considerations', goal='Assess the impact of positional encoding choices on model training efficiency and inference speed.', bullets=['Analyze computational overhead introduced by different encoding schemes', 'Consider memory implications of storing learnable positional vectors versus fixed encodings', 'Discuss trade-offs between model expressiveness and resource consumption'], target_words=220, tags=[], requires_research=False, requires_citations=False, requires_code=False), Task(id=7, title='Debugging and Observability Tips for Positional Encoding Issues', goal='Equip developers with strategies to diagnose and fix issues related to positional encoding implementations.', bullets=['Recommend ways to visualize positional encodings and their effect on embeddings', 'Suggest tests to verify correct addition of positional information into the model', 'Outline debugging approaches for common issues such as sequence length mismatches or embedding misalignments'], target_words=230, tags=[], requires_research=False, requires_citations=False, requires_code=False)]),\n",
              " 'sections': [(1,\n",
              "   '## Introduction to the Need for Positional Encoding in Transformers\\n\\nTransformers rely heavily on self-attention mechanisms, which allow the model to weigh the importance of different tokens in the input sequence relative to each other. However, this self-attention process treats input tokens as a set rather than a sequence, meaning it inherently lacks awareness of the order in which tokens appear. Without any positional information, the model cannot distinguish between sequences that contain the same tokens but in different orders.\\n\\nSequence order is crucial in natural language processing and other sequential data tasks because the meaning often depends on the arrangement of elements. For example, the sentences “The cat sat on the mat” and “On the mat sat the cat” contain the same words but convey different syntactic structures and emphases. Similarly, in time-series or speech data, the order of events influences interpretation and prediction.\\n\\nUnlike transformers, recurrent neural networks (RNNs) and convolutional neural networks (CNNs) inherently encode sequence order through their architectures. RNNs process tokens sequentially, maintaining a hidden state that evolves with each step, thereby capturing order information. CNNs use localized filters that slide over the input, embedding relative positional context within their receptive fields. Transformers, lacking these sequential or localized operations, require an explicit method—positional encoding—to inject order information into their input representations.'),\n",
              "  (2,\n",
              "   '## Fundamentals of Positional Encoding\\n\\nPositional encoding is a crucial technique used in transformer architectures to inject information about the order of tokens in a sequence. Unlike recurrent or convolutional networks, transformers process input tokens in parallel, which means they lack inherent awareness of token positions. Positional encoding addresses this by adding sequence order information directly to the input embeddings, enabling the model to capture the relative and absolute positions of tokens.\\n\\nKey requirements for positional encoding include:\\n\\n- **Fixed or Learnable:** Positional encodings can either be pre-defined using fixed functions like sine and cosine waves or learned parameters updated during training. Both approaches aim to provide meaningful positional signals.\\n- **Differentiable:** The encoding method must be differentiable to allow gradient-based optimization during backpropagation.\\n- **Compatibility with Input Embeddings:** Positional encodings are added or concatenated to the input embeddings, so they must match the dimensionality and scale of the token embeddings for seamless integration.\\n\\nTypically, the dimensionality of positional encodings aligns exactly with the embedding size used for tokens. For example, if each token is represented by a 512-dimensional vector, the positional encoding will also be a 512-dimensional vector. This alignment allows element-wise addition, preserving the embedding dimensions while augmenting them with position information. By combining token embeddings with positional encodings, transformers gain the ability to distinguish tokens based on their order, which is vital for understanding context in sequential data.'),\n",
              "  (3,\n",
              "   '## Sinusoidal Positional Encoding Explained\\n\\nThe original Transformer architecture uses sinusoidal positional encoding to inject information about the position of tokens within a sequence. Unlike recurrent or convolutional models, Transformers process all tokens simultaneously, so positional information must be explicitly added to the input embeddings.\\n\\nFor each position \\\\( pos \\\\) and dimension \\\\( i \\\\) of the positional encoding vector, the encoding is defined by sinusoidal functions as follows:\\n\\n\\\\[\\n\\\\text{PE}_{(pos, 2i)} = \\\\sin\\\\left(\\\\frac{pos}{10000^{\\\\frac{2i}{d_{\\\\text{model}}}}}\\\\right)\\n\\\\]\\n\\\\[\\n\\\\text{PE}_{(pos, 2i+1)} = \\\\cos\\\\left(\\\\frac{pos}{10000^{\\\\frac{2i}{d_{\\\\text{model}}}}}\\\\right)\\n\\\\]\\n\\nHere, \\\\( d_{\\\\text{model}} \\\\) is the dimensionality of the model embeddings, and the encoding alternates sine and cosine for even and odd dimensions respectively. This design creates a unique encoding for each position where each dimension corresponds to a sinusoid of different wavelength.\\n\\nMathematically, this formulation allows the model to learn to attend to relative positions because any linear function of position can be represented as a linear combination of these sinusoids. Furthermore, since the wavelengths range from \\\\( 2\\\\pi \\\\) to \\\\( 10000 \\\\times 2\\\\pi \\\\), the model can generalize to sequences longer than those seen during training by extrapolating the positional patterns.\\n\\nAdvantages of sinusoidal positional encoding include:\\n\\n- **Fixed Encoding**: The encoding is deterministic and does not require learning additional parameters, reducing model complexity.\\n- **Smoothness**: The continuous nature of sine and cosine functions ensures smooth transitions between positions, aiding gradient-based optimization.\\n- **Interpretability**: The encoding reveals relative position information naturally, as sinusoidal functions encode periodicity and distance in a way that the model can exploit for attention.\\n\\nHere is a minimal Python example to generate sinusoidal positional encodings:\\n\\n```python\\nimport numpy as np\\n\\ndef get_sinusoidal_positional_encoding(seq_len, d_model):\\n    pos = np.arange(seq_len)[:, np.newaxis]\\n    i = np.arange(d_model)[np.newaxis, :]\\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\\n    angle_rads = pos * angle_rates\\n\\n    # apply sin to even indices; cos to odd indices\\n    pos_encoding = np.zeros_like(angle_rads)\\n    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\\n    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\\n    return pos_encoding\\n\\n# Example usage\\npos_encoding = get_sinusoidal_positional_encoding(seq_len=50, d_model=512)\\nprint(pos_encoding.shape)  # (50, 512)\\n```\\n\\nThis approach ensures the Transformer model has a meaningful and generalizable representation of token positions without adding trainable parameters.'),\n",
              "  (4,\n",
              "   '## Learnable Positional Embeddings\\n\\nLearnable positional embeddings represent an alternative to fixed positional encodings by treating position information as parameter vectors that are learned during the model’s training process. Instead of using a predefined function like sinusoidal waves to encode position, the model initializes a set of embedding vectors—one per position up to a maximum sequence length. These embeddings are updated through backpropagation alongside the rest of the model parameters, allowing the network to optimize position representations specific to the task and data.\\n\\nCompared to sinusoidal encodings, learnable positional embeddings offer greater flexibility. Because they are not constrained to follow a fixed mathematical form, the model can adapt these embeddings to better capture positional dependencies that are relevant to the dataset. However, this flexibility comes at the cost of potentially reduced generalization to sequence lengths beyond those seen during training. Sinusoidal encodings, by contrast, provide a continuous and extrapolatable representation that can generalize to arbitrary sequence lengths without retraining.\\n\\nFrom an implementation standpoint, learnable positional embeddings are straightforward: they are typically implemented as an embedding matrix indexed by position. In frameworks like PyTorch or TensorFlow, this can be realized with an `nn.Embedding` or `tf.keras.layers.Embedding` layer. One consideration is to set a maximum sequence length, since the embedding matrix size is fixed and cannot easily handle longer sequences without retraining or resizing.\\n\\nModern transformer architectures often use learnable positional embeddings when the maximum sequence length is known and fixed, such as in language models trained on fixed-length inputs. This approach allows the model to fully leverage task-specific positional patterns. In contrast, models that require handling arbitrary sequence lengths or better extrapolation might prefer fixed sinusoidal encodings. Both methods remain widely used, and the choice depends on the specific application and performance trade-offs.'),\n",
              "  (5,\n",
              "   \"## Edge Cases and Limitations of Positional Encoding Methods\\n\\nPositional encoding methods in transformers, whether fixed or learnable, face several challenges that can impact model effectiveness, especially in edge cases.\\n\\nFirst, learnable positional embeddings typically have a fixed maximum length determined during training. This limitation means that if the model encounters sequences longer than this length during inference, it lacks embeddings for those out-of-vocabulary positions. As a result, the model cannot accurately represent positions beyond its training scope, potentially degrading performance or requiring fallback strategies like truncation or interpolation.\\n\\nSecond, generalization to longer sequences than seen during training is a significant challenge. Fixed sinusoidal encodings theoretically extend to arbitrary lengths due to their continuous functional form, but in practice, their effectiveness can diminish as the sequence length grows far beyond training examples. Learnable embeddings, on the other hand, do not naturally generalize since they are discrete vectors tied to specific positions, making extrapolation impossible without additional mechanisms.\\n\\nFinally, positional encodings are susceptible to noise or corruption. If positional information is perturbed—due to input corruption, adversarial attacks, or implementation errors—the model's ability to discern token order can be impaired. This disruption can cause the attention mechanism to attend incorrectly, leading to degraded contextual understanding and performance drops. Robustness to such noise remains an open research area, with some approaches exploring regularization or alternative encoding schemes to mitigate sensitivity.\\n\\nOverall, while positional encodings enable transformers to capture sequence order, their fixed length, generalization limits, and vulnerability to noise highlight important practical considerations for developers designing and deploying transformer models.\"),\n",
              "  (6,\n",
              "   \"## Performance and Computational Considerations\\n\\nPositional encoding schemes directly influence the computational efficiency of transformer models during both training and inference. Fixed positional encodings, such as sinusoidal functions, introduce minimal overhead since they can be computed on the fly or precomputed once and reused without additional parameters. This results in a lightweight addition that does not significantly impact training speed or memory usage.\\n\\nIn contrast, learnable positional embeddings require storing additional parameters proportional to the maximum input length and embedding dimension. This increases the model’s memory footprint, especially for long sequences, impacting GPU memory consumption during training. Moreover, since these embeddings are updated via backpropagation, they add to the parameter optimization workload, potentially slowing down convergence slightly.\\n\\nThe choice between fixed and learnable encodings presents a trade-off: learnable embeddings can adapt to specific data distributions, potentially improving model expressiveness and accuracy. However, this comes at the cost of increased resource consumption and longer training times. Fixed encodings are more computationally efficient but may limit the model's ability to capture complex positional patterns.\\n\\nUltimately, the decision depends on application requirements. For scenarios demanding fast inference and lower resource usage, sinusoidal or other fixed encodings are preferable. For tasks where capturing nuanced positional relationships is critical, investing in learnable positional vectors may justify the additional computational cost.\"),\n",
              "  (7,\n",
              "   '## Debugging and Observability Tips for Positional Encoding Issues\\n\\nWhen working with positional encoding in transformer models, effective debugging and observability are crucial to ensure the model correctly incorporates positional information.\\n\\n- **Visualizing Positional Encodings**: Plot the positional encoding vectors to inspect their patterns and variance across positions. Visualizations such as heatmaps can reveal whether the encodings follow expected sinusoidal patterns or learned embeddings exhibit meaningful diversity. Overlay positional encodings with token embeddings to verify how they combine visually.\\n\\n- **Testing Correct Addition of Positional Information**: Implement unit tests that confirm positional encodings are added element-wise to token embeddings before feeding into the transformer layers. For example, test with known inputs and positional indices to check if the resulting embeddings differ appropriately at different positions. Compare outputs with and without positional encodings to ensure positional information impacts downstream layers.\\n\\n- **Debugging Common Issues**:\\n  - *Sequence Length Mismatches*: Confirm that the positional encoding matrix matches the input sequence length. Mismatches can cause broadcasting errors or silent misalignments.\\n  - *Embedding Dimension Alignment*: Verify that positional encodings and token embeddings share the same dimensionality. A dimensionality mismatch will raise errors or lead to incorrect addition.\\n  - *Indexing Errors*: Check the correctness of position indices used when generating or applying encodings, ensuring zero-based or one-based indexing does not cause off-by-one errors.\\n  - *Unexpected Zero or Constant Vectors*: If positional encodings appear too uniform or zeroed out, investigate initialization and update logic, especially when using learned positional embeddings.\\n\\nBy combining visualization, targeted tests, and careful validation of dimensions and indices, developers can more quickly identify and resolve positional encoding issues in transformer implementations.'),\n",
              "  (1,\n",
              "   '## Introduction to the Need for Positional Encoding in Transformers\\n\\nTransformers rely heavily on self-attention mechanisms, which allow the model to weigh the importance of different tokens in the input sequence relative to each other. However, this self-attention process treats input tokens as a set rather than a sequence, meaning it inherently lacks awareness of the order in which tokens appear. Without any positional information, the model cannot distinguish between sequences that contain the same tokens but in different orders.\\n\\nSequence order is crucial in natural language processing and other sequential data tasks because the meaning often depends on the arrangement of elements. For example, the sentences “The cat sat on the mat” and “On the mat sat the cat” contain the same words but convey different syntactic structures and emphases. Similarly, in time-series or speech data, the order of events influences interpretation and prediction.\\n\\nUnlike transformers, recurrent neural networks (RNNs) and convolutional neural networks (CNNs) inherently encode sequence order through their architectures. RNNs process tokens sequentially, maintaining a hidden state that evolves with each step, thereby capturing order information. CNNs use localized filters that slide over the input, embedding relative positional context within their receptive fields. Transformers, lacking these sequential or localized operations, require an explicit method—positional encoding—to inject order information into their input representations.'),\n",
              "  (2,\n",
              "   '## Fundamentals of Positional Encoding\\n\\nPositional encoding is a crucial technique used in transformer architectures to inject information about the order of tokens in a sequence. Unlike recurrent or convolutional networks, transformers process input tokens in parallel, which means they lack inherent awareness of token positions. Positional encoding addresses this by adding sequence order information directly to the input embeddings, enabling the model to capture the relative and absolute positions of tokens.\\n\\nKey requirements for positional encoding include:\\n\\n- **Fixed or Learnable:** Positional encodings can either be pre-defined using fixed functions like sine and cosine waves or learned parameters updated during training. Both approaches aim to provide meaningful positional signals.\\n- **Differentiable:** The encoding method must be differentiable to allow gradient-based optimization during backpropagation.\\n- **Compatibility with Input Embeddings:** Positional encodings are added or concatenated to the input embeddings, so they must match the dimensionality and scale of the token embeddings for seamless integration.\\n\\nTypically, the dimensionality of positional encodings aligns exactly with the embedding size used for tokens. For example, if each token is represented by a 512-dimensional vector, the positional encoding will also be a 512-dimensional vector. This alignment allows element-wise addition, preserving the embedding dimensions while augmenting them with position information. By combining token embeddings with positional encodings, transformers gain the ability to distinguish tokens based on their order, which is vital for understanding context in sequential data.'),\n",
              "  (3,\n",
              "   '## Sinusoidal Positional Encoding Explained\\n\\nThe original Transformer architecture uses sinusoidal positional encoding to inject information about the position of tokens within a sequence. Unlike recurrent or convolutional models, Transformers process all tokens simultaneously, so positional information must be explicitly added to the input embeddings.\\n\\nFor each position \\\\( pos \\\\) and dimension \\\\( i \\\\) of the positional encoding vector, the encoding is defined by sinusoidal functions as follows:\\n\\n\\\\[\\n\\\\text{PE}_{(pos, 2i)} = \\\\sin\\\\left(\\\\frac{pos}{10000^{\\\\frac{2i}{d_{\\\\text{model}}}}}\\\\right)\\n\\\\]\\n\\\\[\\n\\\\text{PE}_{(pos, 2i+1)} = \\\\cos\\\\left(\\\\frac{pos}{10000^{\\\\frac{2i}{d_{\\\\text{model}}}}}\\\\right)\\n\\\\]\\n\\nHere, \\\\( d_{\\\\text{model}} \\\\) is the dimensionality of the model embeddings, and the encoding alternates sine and cosine for even and odd dimensions respectively. This design creates a unique encoding for each position where each dimension corresponds to a sinusoid of different wavelength.\\n\\nMathematically, this formulation allows the model to learn to attend to relative positions because any linear function of position can be represented as a linear combination of these sinusoids. Furthermore, since the wavelengths range from \\\\( 2\\\\pi \\\\) to \\\\( 10000 \\\\times 2\\\\pi \\\\), the model can generalize to sequences longer than those seen during training by extrapolating the positional patterns.\\n\\nAdvantages of sinusoidal positional encoding include:\\n\\n- **Fixed Encoding**: The encoding is deterministic and does not require learning additional parameters, reducing model complexity.\\n- **Smoothness**: The continuous nature of sine and cosine functions ensures smooth transitions between positions, aiding gradient-based optimization.\\n- **Interpretability**: The encoding reveals relative position information naturally, as sinusoidal functions encode periodicity and distance in a way that the model can exploit for attention.\\n\\nHere is a minimal Python example to generate sinusoidal positional encodings:\\n\\n```python\\nimport numpy as np\\n\\ndef get_sinusoidal_positional_encoding(seq_len, d_model):\\n    pos = np.arange(seq_len)[:, np.newaxis]\\n    i = np.arange(d_model)[np.newaxis, :]\\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\\n    angle_rads = pos * angle_rates\\n\\n    # apply sin to even indices; cos to odd indices\\n    pos_encoding = np.zeros_like(angle_rads)\\n    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\\n    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\\n    return pos_encoding\\n\\n# Example usage\\npos_encoding = get_sinusoidal_positional_encoding(seq_len=50, d_model=512)\\nprint(pos_encoding.shape)  # (50, 512)\\n```\\n\\nThis approach ensures the Transformer model has a meaningful and generalizable representation of token positions without adding trainable parameters.'),\n",
              "  (4,\n",
              "   '## Learnable Positional Embeddings\\n\\nLearnable positional embeddings represent an alternative to fixed positional encodings by treating position information as parameter vectors that are learned during the model’s training process. Instead of using a predefined function like sinusoidal waves to encode position, the model initializes a set of embedding vectors—one per position up to a maximum sequence length. These embeddings are updated through backpropagation alongside the rest of the model parameters, allowing the network to optimize position representations specific to the task and data.\\n\\nCompared to sinusoidal encodings, learnable positional embeddings offer greater flexibility. Because they are not constrained to follow a fixed mathematical form, the model can adapt these embeddings to better capture positional dependencies that are relevant to the dataset. However, this flexibility comes at the cost of potentially reduced generalization to sequence lengths beyond those seen during training. Sinusoidal encodings, by contrast, provide a continuous and extrapolatable representation that can generalize to arbitrary sequence lengths without retraining.\\n\\nFrom an implementation standpoint, learnable positional embeddings are straightforward: they are typically implemented as an embedding matrix indexed by position. In frameworks like PyTorch or TensorFlow, this can be realized with an `nn.Embedding` or `tf.keras.layers.Embedding` layer. One consideration is to set a maximum sequence length, since the embedding matrix size is fixed and cannot easily handle longer sequences without retraining or resizing.\\n\\nModern transformer architectures often use learnable positional embeddings when the maximum sequence length is known and fixed, such as in language models trained on fixed-length inputs. This approach allows the model to fully leverage task-specific positional patterns. In contrast, models that require handling arbitrary sequence lengths or better extrapolation might prefer fixed sinusoidal encodings. Both methods remain widely used, and the choice depends on the specific application and performance trade-offs.'),\n",
              "  (5,\n",
              "   \"## Edge Cases and Limitations of Positional Encoding Methods\\n\\nPositional encoding methods in transformers, whether fixed or learnable, face several challenges that can impact model effectiveness, especially in edge cases.\\n\\nFirst, learnable positional embeddings typically have a fixed maximum length determined during training. This limitation means that if the model encounters sequences longer than this length during inference, it lacks embeddings for those out-of-vocabulary positions. As a result, the model cannot accurately represent positions beyond its training scope, potentially degrading performance or requiring fallback strategies like truncation or interpolation.\\n\\nSecond, generalization to longer sequences than seen during training is a significant challenge. Fixed sinusoidal encodings theoretically extend to arbitrary lengths due to their continuous functional form, but in practice, their effectiveness can diminish as the sequence length grows far beyond training examples. Learnable embeddings, on the other hand, do not naturally generalize since they are discrete vectors tied to specific positions, making extrapolation impossible without additional mechanisms.\\n\\nFinally, positional encodings are susceptible to noise or corruption. If positional information is perturbed—due to input corruption, adversarial attacks, or implementation errors—the model's ability to discern token order can be impaired. This disruption can cause the attention mechanism to attend incorrectly, leading to degraded contextual understanding and performance drops. Robustness to such noise remains an open research area, with some approaches exploring regularization or alternative encoding schemes to mitigate sensitivity.\\n\\nOverall, while positional encodings enable transformers to capture sequence order, their fixed length, generalization limits, and vulnerability to noise highlight important practical considerations for developers designing and deploying transformer models.\"),\n",
              "  (6,\n",
              "   \"## Performance and Computational Considerations\\n\\nPositional encoding schemes directly influence the computational efficiency of transformer models during both training and inference. Fixed positional encodings, such as sinusoidal functions, introduce minimal overhead since they can be computed on the fly or precomputed once and reused without additional parameters. This results in a lightweight addition that does not significantly impact training speed or memory usage.\\n\\nIn contrast, learnable positional embeddings require storing additional parameters proportional to the maximum input length and embedding dimension. This increases the model’s memory footprint, especially for long sequences, impacting GPU memory consumption during training. Moreover, since these embeddings are updated via backpropagation, they add to the parameter optimization workload, potentially slowing down convergence slightly.\\n\\nThe choice between fixed and learnable encodings presents a trade-off: learnable embeddings can adapt to specific data distributions, potentially improving model expressiveness and accuracy. However, this comes at the cost of increased resource consumption and longer training times. Fixed encodings are more computationally efficient but may limit the model's ability to capture complex positional patterns.\\n\\nUltimately, the decision depends on application requirements. For scenarios demanding fast inference and lower resource usage, sinusoidal or other fixed encodings are preferable. For tasks where capturing nuanced positional relationships is critical, investing in learnable positional vectors may justify the additional computational cost.\"),\n",
              "  (7,\n",
              "   '## Debugging and Observability Tips for Positional Encoding Issues\\n\\nWhen working with positional encoding in transformer models, effective debugging and observability are crucial to ensure the model correctly incorporates positional information.\\n\\n- **Visualizing Positional Encodings**: Plot the positional encoding vectors to inspect their patterns and variance across positions. Visualizations such as heatmaps can reveal whether the encodings follow expected sinusoidal patterns or learned embeddings exhibit meaningful diversity. Overlay positional encodings with token embeddings to verify how they combine visually.\\n\\n- **Testing Correct Addition of Positional Information**: Implement unit tests that confirm positional encodings are added element-wise to token embeddings before feeding into the transformer layers. For example, test with known inputs and positional indices to check if the resulting embeddings differ appropriately at different positions. Compare outputs with and without positional encodings to ensure positional information impacts downstream layers.\\n\\n- **Debugging Common Issues**:\\n  - *Sequence Length Mismatches*: Confirm that the positional encoding matrix matches the input sequence length. Mismatches can cause broadcasting errors or silent misalignments.\\n  - *Embedding Dimension Alignment*: Verify that positional encodings and token embeddings share the same dimensionality. A dimensionality mismatch will raise errors or lead to incorrect addition.\\n  - *Indexing Errors*: Check the correctness of position indices used when generating or applying encodings, ensuring zero-based or one-based indexing does not cause off-by-one errors.\\n  - *Unexpected Zero or Constant Vectors*: If positional encodings appear too uniform or zeroed out, investigate initialization and update logic, especially when using learned positional embeddings.\\n\\nBy combining visualization, targeted tests, and careful validation of dimensions and indices, developers can more quickly identify and resolve positional encoding issues in transformer implementations.')],\n",
              " 'merged_md': \"# Positional Encoding in Transformer Architecture\\n\\n## Introduction to the Need for Positional Encoding in Transformers\\n\\nTransformers rely heavily on self-attention mechanisms, which allow the model to weigh the importance of different tokens in the input sequence relative to each other. However, this self-attention process treats input tokens as a set rather than a sequence, meaning it inherently lacks awareness of the order in which tokens appear. Without any positional information, the model cannot distinguish between sequences that contain the same tokens but in different orders.\\n\\nSequence order is crucial in natural language processing and other sequential data tasks because the meaning often depends on the arrangement of elements. For example, the sentences “The cat sat on the mat” and “On the mat sat the cat” contain the same words but convey different syntactic structures and emphases. Similarly, in time-series or speech data, the order of events influences interpretation and prediction.\\n\\nUnlike transformers, recurrent neural networks (RNNs) and convolutional neural networks (CNNs) inherently encode sequence order through their architectures. RNNs process tokens sequentially, maintaining a hidden state that evolves with each step, thereby capturing order information. CNNs use localized filters that slide over the input, embedding relative positional context within their receptive fields. Transformers, lacking these sequential or localized operations, require an explicit method—positional encoding—to inject order information into their input representations.\\n\\n## Fundamentals of Positional Encoding\\n\\nPositional encoding is a crucial technique used in transformer architectures to inject information about the order of tokens in a sequence. Unlike recurrent or convolutional networks, transformers process input tokens in parallel, which means they lack inherent awareness of token positions. Positional encoding addresses this by adding sequence order information directly to the input embeddings, enabling the model to capture the relative and absolute positions of tokens.\\n\\nKey requirements for positional encoding include:\\n\\n- **Fixed or Learnable:** Positional encodings can either be pre-defined using fixed functions like sine and cosine waves or learned parameters updated during training. Both approaches aim to provide meaningful positional signals.\\n- **Differentiable:** The encoding method must be differentiable to allow gradient-based optimization during backpropagation.\\n- **Compatibility with Input Embeddings:** Positional encodings are added or concatenated to the input embeddings, so they must match the dimensionality and scale of the token embeddings for seamless integration.\\n\\nTypically, the dimensionality of positional encodings aligns exactly with the embedding size used for tokens. For example, if each token is represented by a 512-dimensional vector, the positional encoding will also be a 512-dimensional vector. This alignment allows element-wise addition, preserving the embedding dimensions while augmenting them with position information. By combining token embeddings with positional encodings, transformers gain the ability to distinguish tokens based on their order, which is vital for understanding context in sequential data.\\n\\n## Sinusoidal Positional Encoding Explained\\n\\nThe original Transformer architecture uses sinusoidal positional encoding to inject information about the position of tokens within a sequence. Unlike recurrent or convolutional models, Transformers process all tokens simultaneously, so positional information must be explicitly added to the input embeddings.\\n\\nFor each position \\\\( pos \\\\) and dimension \\\\( i \\\\) of the positional encoding vector, the encoding is defined by sinusoidal functions as follows:\\n\\n\\\\[\\n\\\\text{PE}_{(pos, 2i)} = \\\\sin\\\\left(\\\\frac{pos}{10000^{\\\\frac{2i}{d_{\\\\text{model}}}}}\\\\right)\\n\\\\]\\n\\\\[\\n\\\\text{PE}_{(pos, 2i+1)} = \\\\cos\\\\left(\\\\frac{pos}{10000^{\\\\frac{2i}{d_{\\\\text{model}}}}}\\\\right)\\n\\\\]\\n\\nHere, \\\\( d_{\\\\text{model}} \\\\) is the dimensionality of the model embeddings, and the encoding alternates sine and cosine for even and odd dimensions respectively. This design creates a unique encoding for each position where each dimension corresponds to a sinusoid of different wavelength.\\n\\nMathematically, this formulation allows the model to learn to attend to relative positions because any linear function of position can be represented as a linear combination of these sinusoids. Furthermore, since the wavelengths range from \\\\( 2\\\\pi \\\\) to \\\\( 10000 \\\\times 2\\\\pi \\\\), the model can generalize to sequences longer than those seen during training by extrapolating the positional patterns.\\n\\nAdvantages of sinusoidal positional encoding include:\\n\\n- **Fixed Encoding**: The encoding is deterministic and does not require learning additional parameters, reducing model complexity.\\n- **Smoothness**: The continuous nature of sine and cosine functions ensures smooth transitions between positions, aiding gradient-based optimization.\\n- **Interpretability**: The encoding reveals relative position information naturally, as sinusoidal functions encode periodicity and distance in a way that the model can exploit for attention.\\n\\nHere is a minimal Python example to generate sinusoidal positional encodings:\\n\\n```python\\nimport numpy as np\\n\\ndef get_sinusoidal_positional_encoding(seq_len, d_model):\\n    pos = np.arange(seq_len)[:, np.newaxis]\\n    i = np.arange(d_model)[np.newaxis, :]\\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\\n    angle_rads = pos * angle_rates\\n\\n    # apply sin to even indices; cos to odd indices\\n    pos_encoding = np.zeros_like(angle_rads)\\n    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\\n    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\\n    return pos_encoding\\n\\n# Example usage\\npos_encoding = get_sinusoidal_positional_encoding(seq_len=50, d_model=512)\\nprint(pos_encoding.shape)  # (50, 512)\\n```\\n\\nThis approach ensures the Transformer model has a meaningful and generalizable representation of token positions without adding trainable parameters.\\n\\n## Learnable Positional Embeddings\\n\\nLearnable positional embeddings represent an alternative to fixed positional encodings by treating position information as parameter vectors that are learned during the model’s training process. Instead of using a predefined function like sinusoidal waves to encode position, the model initializes a set of embedding vectors—one per position up to a maximum sequence length. These embeddings are updated through backpropagation alongside the rest of the model parameters, allowing the network to optimize position representations specific to the task and data.\\n\\nCompared to sinusoidal encodings, learnable positional embeddings offer greater flexibility. Because they are not constrained to follow a fixed mathematical form, the model can adapt these embeddings to better capture positional dependencies that are relevant to the dataset. However, this flexibility comes at the cost of potentially reduced generalization to sequence lengths beyond those seen during training. Sinusoidal encodings, by contrast, provide a continuous and extrapolatable representation that can generalize to arbitrary sequence lengths without retraining.\\n\\nFrom an implementation standpoint, learnable positional embeddings are straightforward: they are typically implemented as an embedding matrix indexed by position. In frameworks like PyTorch or TensorFlow, this can be realized with an `nn.Embedding` or `tf.keras.layers.Embedding` layer. One consideration is to set a maximum sequence length, since the embedding matrix size is fixed and cannot easily handle longer sequences without retraining or resizing.\\n\\nModern transformer architectures often use learnable positional embeddings when the maximum sequence length is known and fixed, such as in language models trained on fixed-length inputs. This approach allows the model to fully leverage task-specific positional patterns. In contrast, models that require handling arbitrary sequence lengths or better extrapolation might prefer fixed sinusoidal encodings. Both methods remain widely used, and the choice depends on the specific application and performance trade-offs.\\n\\n## Edge Cases and Limitations of Positional Encoding Methods\\n\\nPositional encoding methods in transformers, whether fixed or learnable, face several challenges that can impact model effectiveness, especially in edge cases.\\n\\nFirst, learnable positional embeddings typically have a fixed maximum length determined during training. This limitation means that if the model encounters sequences longer than this length during inference, it lacks embeddings for those out-of-vocabulary positions. As a result, the model cannot accurately represent positions beyond its training scope, potentially degrading performance or requiring fallback strategies like truncation or interpolation.\\n\\nSecond, generalization to longer sequences than seen during training is a significant challenge. Fixed sinusoidal encodings theoretically extend to arbitrary lengths due to their continuous functional form, but in practice, their effectiveness can diminish as the sequence length grows far beyond training examples. Learnable embeddings, on the other hand, do not naturally generalize since they are discrete vectors tied to specific positions, making extrapolation impossible without additional mechanisms.\\n\\nFinally, positional encodings are susceptible to noise or corruption. If positional information is perturbed—due to input corruption, adversarial attacks, or implementation errors—the model's ability to discern token order can be impaired. This disruption can cause the attention mechanism to attend incorrectly, leading to degraded contextual understanding and performance drops. Robustness to such noise remains an open research area, with some approaches exploring regularization or alternative encoding schemes to mitigate sensitivity.\\n\\nOverall, while positional encodings enable transformers to capture sequence order, their fixed length, generalization limits, and vulnerability to noise highlight important practical considerations for developers designing and deploying transformer models.\\n\\n## Performance and Computational Considerations\\n\\nPositional encoding schemes directly influence the computational efficiency of transformer models during both training and inference. Fixed positional encodings, such as sinusoidal functions, introduce minimal overhead since they can be computed on the fly or precomputed once and reused without additional parameters. This results in a lightweight addition that does not significantly impact training speed or memory usage.\\n\\nIn contrast, learnable positional embeddings require storing additional parameters proportional to the maximum input length and embedding dimension. This increases the model’s memory footprint, especially for long sequences, impacting GPU memory consumption during training. Moreover, since these embeddings are updated via backpropagation, they add to the parameter optimization workload, potentially slowing down convergence slightly.\\n\\nThe choice between fixed and learnable encodings presents a trade-off: learnable embeddings can adapt to specific data distributions, potentially improving model expressiveness and accuracy. However, this comes at the cost of increased resource consumption and longer training times. Fixed encodings are more computationally efficient but may limit the model's ability to capture complex positional patterns.\\n\\nUltimately, the decision depends on application requirements. For scenarios demanding fast inference and lower resource usage, sinusoidal or other fixed encodings are preferable. For tasks where capturing nuanced positional relationships is critical, investing in learnable positional vectors may justify the additional computational cost.\\n\\n## Debugging and Observability Tips for Positional Encoding Issues\\n\\nWhen working with positional encoding in transformer models, effective debugging and observability are crucial to ensure the model correctly incorporates positional information.\\n\\n- **Visualizing Positional Encodings**: Plot the positional encoding vectors to inspect their patterns and variance across positions. Visualizations such as heatmaps can reveal whether the encodings follow expected sinusoidal patterns or learned embeddings exhibit meaningful diversity. Overlay positional encodings with token embeddings to verify how they combine visually.\\n\\n- **Testing Correct Addition of Positional Information**: Implement unit tests that confirm positional encodings are added element-wise to token embeddings before feeding into the transformer layers. For example, test with known inputs and positional indices to check if the resulting embeddings differ appropriately at different positions. Compare outputs with and without positional encodings to ensure positional information impacts downstream layers.\\n\\n- **Debugging Common Issues**:\\n  - *Sequence Length Mismatches*: Confirm that the positional encoding matrix matches the input sequence length. Mismatches can cause broadcasting errors or silent misalignments.\\n  - *Embedding Dimension Alignment*: Verify that positional encodings and token embeddings share the same dimensionality. A dimensionality mismatch will raise errors or lead to incorrect addition.\\n  - *Indexing Errors*: Check the correctness of position indices used when generating or applying encodings, ensuring zero-based or one-based indexing does not cause off-by-one errors.\\n  - *Unexpected Zero or Constant Vectors*: If positional encodings appear too uniform or zeroed out, investigate initialization and update logic, especially when using learned positional embeddings.\\n\\nBy combining visualization, targeted tests, and careful validation of dimensions and indices, developers can more quickly identify and resolve positional encoding issues in transformer implementations.\\n\",\n",
              " 'md_with_placeholders': \"# Positional Encoding in Transformer Architecture\\n\\n## Introduction to the Need for Positional Encoding in Transformers\\n\\nTransformers rely heavily on self-attention mechanisms, which allow the model to weigh the importance of different tokens in the input sequence relative to each other. However, this self-attention process treats input tokens as a set rather than a sequence, meaning it inherently lacks awareness of the order in which tokens appear. Without any positional information, the model cannot distinguish between sequences that contain the same tokens but in different orders.\\n\\nSequence order is crucial in natural language processing and other sequential data tasks because the meaning often depends on the arrangement of elements. For example, the sentences “The cat sat on the mat” and “On the mat sat the cat” contain the same words but convey different syntactic structures and emphases. Similarly, in time-series or speech data, the order of events influences interpretation and prediction.\\n\\nUnlike transformers, recurrent neural networks (RNNs) and convolutional neural networks (CNNs) inherently encode sequence order through their architectures. RNNs process tokens sequentially, maintaining a hidden state that evolves with each step, thereby capturing order information. CNNs use localized filters that slide over the input, embedding relative positional context within their receptive fields. Transformers, lacking these sequential or localized operations, require an explicit method—positional encoding—to inject order information into their input representations.\\n\\n[[IMAGE_1]]\\n\\n## Fundamentals of Positional Encoding\\n\\nPositional encoding is a crucial technique used in transformer architectures to inject information about the order of tokens in a sequence. Unlike recurrent or convolutional networks, transformers process input tokens in parallel, which means they lack inherent awareness of token positions. Positional encoding addresses this by adding sequence order information directly to the input embeddings, enabling the model to capture the relative and absolute positions of tokens.\\n\\nKey requirements for positional encoding include:\\n\\n- **Fixed or Learnable:** Positional encodings can either be pre-defined using fixed functions like sine and cosine waves or learned parameters updated during training. Both approaches aim to provide meaningful positional signals.\\n- **Differentiable:** The encoding method must be differentiable to allow gradient-based optimization during backpropagation.\\n- **Compatibility with Input Embeddings:** Positional encodings are added or concatenated to the input embeddings, so they must match the dimensionality and scale of the token embeddings for seamless integration.\\n\\nTypically, the dimensionality of positional encodings aligns exactly with the embedding size used for tokens. For example, if each token is represented by a 512-dimensional vector, the positional encoding will also be a 512-dimensional vector. This alignment allows element-wise addition, preserving the embedding dimensions while augmenting them with position information. By combining token embeddings with positional encodings, transformers gain the ability to distinguish tokens based on their order, which is vital for understanding context in sequential data.\\n\\n## Sinusoidal Positional Encoding Explained\\n\\nThe original Transformer architecture uses sinusoidal positional encoding to inject information about the position of tokens within a sequence. Unlike recurrent or convolutional models, Transformers process all tokens simultaneously, so positional information must be explicitly added to the input embeddings.\\n\\nFor each position \\\\( pos \\\\) and dimension \\\\( i \\\\) of the positional encoding vector, the encoding is defined by sinusoidal functions as follows:\\n\\n\\\\[\\n\\\\text{PE}_{(pos, 2i)} = \\\\sin\\\\left(\\\\frac{pos}{10000^{\\\\frac{2i}{d_{\\\\text{model}}}}}\\\\right)\\n\\\\]\\n\\\\[\\n\\\\text{PE}_{(pos, 2i+1)} = \\\\cos\\\\left(\\\\frac{pos}{10000^{\\\\frac{2i}{d_{\\\\text{model}}}}}\\\\right)\\n\\\\]\\n\\nHere, \\\\( d_{\\\\text{model}} \\\\) is the dimensionality of the model embeddings, and the encoding alternates sine and cosine for even and odd dimensions respectively. This design creates a unique encoding for each position where each dimension corresponds to a sinusoid of different wavelength.\\n\\nMathematically, this formulation allows the model to learn to attend to relative positions because any linear function of position can be represented as a linear combination of these sinusoids. Furthermore, since the wavelengths range from \\\\( 2\\\\pi \\\\) to \\\\( 10000 \\\\times 2\\\\pi \\\\), the model can generalize to sequences longer than those seen during training by extrapolating the positional patterns.\\n\\nAdvantages of sinusoidal positional encoding include:\\n\\n- **Fixed Encoding**: The encoding is deterministic and does not require learning additional parameters, reducing model complexity.\\n- **Smoothness**: The continuous nature of sine and cosine functions ensures smooth transitions between positions, aiding gradient-based optimization.\\n- **Interpretability**: The encoding reveals relative position information naturally, as sinusoidal functions encode periodicity and distance in a way that the model can exploit for attention.\\n\\nHere is a minimal Python example to generate sinusoidal positional encodings:\\n\\n```python\\nimport numpy as np\\n\\ndef get_sinusoidal_positional_encoding(seq_len, d_model):\\n    pos = np.arange(seq_len)[:, np.newaxis]\\n    i = np.arange(d_model)[np.newaxis, :]\\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\\n    angle_rads = pos * angle_rates\\n\\n    # apply sin to even indices; cos to odd indices\\n    pos_encoding = np.zeros_like(angle_rads)\\n    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\\n    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\\n    return pos_encoding\\n\\n# Example usage\\npos_encoding = get_sinusoidal_positional_encoding(seq_len=50, d_model=512)\\nprint(pos_encoding.shape)  # (50, 512)\\n```\\n\\nThis approach ensures the Transformer model has a meaningful and generalizable representation of token positions without adding trainable parameters.\\n\\n[[IMAGE_2]]\\n\\n## Learnable Positional Embeddings\\n\\nLearnable positional embeddings represent an alternative to fixed positional encodings by treating position information as parameter vectors that are learned during the model’s training process. Instead of using a predefined function like sinusoidal waves to encode position, the model initializes a set of embedding vectors—one per position up to a maximum sequence length. These embeddings are updated through backpropagation alongside the rest of the model parameters, allowing the network to optimize position representations specific to the task and data.\\n\\nCompared to sinusoidal encodings, learnable positional embeddings offer greater flexibility. Because they are not constrained to follow a fixed mathematical form, the model can adapt these embeddings to better capture positional dependencies that are relevant to the dataset. However, this flexibility comes at the cost of potentially reduced generalization to sequence lengths beyond those seen during training. Sinusoidal encodings, by contrast, provide a continuous and extrapolatable representation that can generalize to arbitrary sequence lengths without retraining.\\n\\nFrom an implementation standpoint, learnable positional embeddings are straightforward: they are typically implemented as an embedding matrix indexed by position. In frameworks like PyTorch or TensorFlow, this can be realized with an `nn.Embedding` or `tf.keras.layers.Embedding` layer. One consideration is to set a maximum sequence length, since the embedding matrix size is fixed and cannot easily handle longer sequences without retraining or resizing.\\n\\nModern transformer architectures often use learnable positional embeddings when the maximum sequence length is known and fixed, such as in language models trained on fixed-length inputs. This approach allows the model to fully leverage task-specific positional patterns. In contrast, models that require handling arbitrary sequence lengths or better extrapolation might prefer fixed sinusoidal encodings. Both methods remain widely used, and the choice depends on the specific application and performance trade-offs.\\n\\n## Edge Cases and Limitations of Positional Encoding Methods\\n\\nPositional encoding methods in transformers, whether fixed or learnable, face several challenges that can impact model effectiveness, especially in edge cases.\\n\\nFirst, learnable positional embeddings typically have a fixed maximum length determined during training. This limitation means that if the model encounters sequences longer than this length during inference, it lacks embeddings for those out-of-vocabulary positions. As a result, the model cannot accurately represent positions beyond its training scope, potentially degrading performance or requiring fallback strategies like truncation or interpolation.\\n\\nSecond, generalization to longer sequences than seen during training is a significant challenge. Fixed sinusoidal encodings theoretically extend to arbitrary lengths due to their continuous functional form, but in practice, their effectiveness can diminish as the sequence length grows far beyond training examples. Learnable embeddings, on the other hand, do not naturally generalize since they are discrete vectors tied to specific positions, making extrapolation impossible without additional mechanisms.\\n\\nFinally, positional encodings are susceptible to noise or corruption. If positional information is perturbed—due to input corruption, adversarial attacks, or implementation errors—the model's ability to discern token order can be impaired. This disruption can cause the attention mechanism to attend incorrectly, leading to degraded contextual understanding and performance drops. Robustness to such noise remains an open research area, with some approaches exploring regularization or alternative encoding schemes to mitigate sensitivity.\\n\\nOverall, while positional encodings enable transformers to capture sequence order, their fixed length, generalization limits, and vulnerability to noise highlight important practical considerations for developers designing and deploying transformer models.\\n\\n## Performance and Computational Considerations\\n\\nPositional encoding schemes directly influence the computational efficiency of transformer models during both training and inference. Fixed positional encodings, such as sinusoidal functions, introduce minimal overhead since they can be computed on the fly or precomputed once and reused without additional parameters. This results in a lightweight addition that does not significantly impact training speed or memory usage.\\n\\nIn contrast, learnable positional embeddings require storing additional parameters proportional to the maximum input length and embedding dimension. This increases the model’s memory footprint, especially for long sequences, impacting GPU memory consumption during training. Moreover, since these embeddings are updated via backpropagation, they add to the parameter optimization workload, potentially slowing down convergence slightly.\\n\\nThe choice between fixed and learnable encodings presents a trade-off: learnable embeddings can adapt to specific data distributions, potentially improving model expressiveness and accuracy. However, this comes at the cost of increased resource consumption and longer training times. Fixed encodings are more computationally efficient but may limit the model's ability to capture complex positional patterns.\\n\\nUltimately, the decision depends on application requirements. For scenarios demanding fast inference and lower resource usage, sinusoidal or other fixed encodings are preferable. For tasks where capturing nuanced positional relationships is critical, investing in learnable positional vectors may justify the additional computational cost.\\n\\n## Debugging and Observability Tips for Positional Encoding Issues\\n\\nWhen working with positional encoding in transformer models, effective debugging and observability are crucial to ensure the model correctly incorporates positional information.\\n\\n- **Visualizing Positional Encodings**: Plot the positional encoding vectors to inspect their patterns and variance across positions. Visualizations such as heatmaps can reveal whether the encodings follow expected sinusoidal patterns or learned embeddings exhibit meaningful diversity. Overlay positional encodings with token embeddings to verify how they combine visually.\\n\\n- **Testing Correct Addition of Positional Information**: Implement unit tests that confirm positional encodings are added element-wise to token embeddings before feeding into the transformer layers. For example, test with known inputs and positional indices to check if the resulting embeddings differ appropriately at different positions. Compare outputs with and without positional encodings to ensure positional information impacts downstream layers.\\n\\n- **Debugging Common Issues**:\\n  - *Sequence Length Mismatches*: Confirm that the positional encoding matrix matches the input sequence length. Mismatches can cause broadcasting errors or silent misalignments.\\n  - *Embedding Dimension Alignment*: Verify that positional encodings and token embeddings share the same dimensionality. A dimensionality mismatch will raise errors or lead to incorrect addition.\\n  - *Indexing Errors*: Check the correctness of position indices used when generating or applying encodings, ensuring zero-based or one-based indexing does not cause off-by-one errors.\\n  - *Unexpected Zero or Constant Vectors*: If positional encodings appear too uniform or zeroed out, investigate initialization and update logic, especially when using learned positional embeddings.\\n\\nBy combining visualization, targeted tests, and careful validation of dimensions and indices, developers can more quickly identify and resolve positional encoding issues in transformer implementations.\\n\\n[[IMAGE_3]]\",\n",
              " 'image_specs': [{'placeholder': '[[IMAGE_1]]',\n",
              "   'filename': 'transformer_positional_encoding_concept.png',\n",
              "   'alt': 'Diagram illustrating the difference between self-attention without positional encoding and with positional encoding in a Transformer',\n",
              "   'caption': 'Self-attention treats tokens as a set; positional encoding injects order information.',\n",
              "   'prompt': 'Create a technical diagram showing two sequences of tokens processed by a Transformer: one without positional encoding where token order is ignored, and one with positional encoding where token order is represented. Highlight that self-attention treats tokens as a set and positional encoding adds position information to embeddings.',\n",
              "   'size': '1024x1024',\n",
              "   'quality': 'high'},\n",
              "  {'placeholder': '[[IMAGE_2]]',\n",
              "   'filename': 'sinusoidal_positional_encoding_pattern.png',\n",
              "   'alt': 'Heatmap visualization of sinusoidal positional encoding patterns across different positions and embedding dimensions',\n",
              "   'caption': 'Sinusoidal positional encoding patterns for sequence positions and embedding dimensions.',\n",
              "   'prompt': 'Generate a heatmap visualization showing sinusoidal positional encodings for a sequence of tokens. The x-axis should represent embedding dimensions alternating sine and cosine waves, and the y-axis should represent token positions. Use color gradients to illustrate the sinusoidal patterns clearly.',\n",
              "   'size': '1024x1024',\n",
              "   'quality': 'high'},\n",
              "  {'placeholder': '[[IMAGE_3]]',\n",
              "   'filename': 'positional_encoding_debugging_tips.png',\n",
              "   'alt': 'Diagram summarizing debugging and observability tips for positional encoding in Transformers',\n",
              "   'caption': 'Key debugging and observability tips for positional encoding issues in Transformer models.',\n",
              "   'prompt': 'Create a clear infographic summarizing key debugging tips for positional encoding in Transformer models, including visualizing positional encodings, testing addition to embeddings, and common issues like dimension mismatch and indexing errors. Use simple icons and short labels for each tip.',\n",
              "   'size': '1024x1024',\n",
              "   'quality': 'high'}],\n",
              " 'final': \"# Positional Encoding in Transformer Architecture\\n\\n## Introduction to the Need for Positional Encoding in Transformers\\n\\nTransformers rely heavily on self-attention mechanisms, which allow the model to weigh the importance of different tokens in the input sequence relative to each other. However, this self-attention process treats input tokens as a set rather than a sequence, meaning it inherently lacks awareness of the order in which tokens appear. Without any positional information, the model cannot distinguish between sequences that contain the same tokens but in different orders.\\n\\nSequence order is crucial in natural language processing and other sequential data tasks because the meaning often depends on the arrangement of elements. For example, the sentences “The cat sat on the mat” and “On the mat sat the cat” contain the same words but convey different syntactic structures and emphases. Similarly, in time-series or speech data, the order of events influences interpretation and prediction.\\n\\nUnlike transformers, recurrent neural networks (RNNs) and convolutional neural networks (CNNs) inherently encode sequence order through their architectures. RNNs process tokens sequentially, maintaining a hidden state that evolves with each step, thereby capturing order information. CNNs use localized filters that slide over the input, embedding relative positional context within their receptive fields. Transformers, lacking these sequential or localized operations, require an explicit method—positional encoding—to inject order information into their input representations.\\n\\n> **[IMAGE GENERATION FAILED]** Self-attention treats tokens as a set; positional encoding injects order information.\\n>\\n> **Alt:** Diagram illustrating the difference between self-attention without positional encoding and with positional encoding in a Transformer\\n>\\n> **Prompt:** Create a technical diagram showing two sequences of tokens processed by a Transformer: one without positional encoding where token order is ignored, and one with positional encoding where token order is represented. Highlight that self-attention treats tokens as a set and positional encoding adds position information to embeddings.\\n>\\n> **Error:** 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-flash-preview-image\\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-flash-preview-image\\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-flash-preview-image\\\\nPlease retry in 47.884324799s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-preview-image'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash-preview-image', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash-preview-image', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '47s'}]}}\\n\\n\\n## Fundamentals of Positional Encoding\\n\\nPositional encoding is a crucial technique used in transformer architectures to inject information about the order of tokens in a sequence. Unlike recurrent or convolutional networks, transformers process input tokens in parallel, which means they lack inherent awareness of token positions. Positional encoding addresses this by adding sequence order information directly to the input embeddings, enabling the model to capture the relative and absolute positions of tokens.\\n\\nKey requirements for positional encoding include:\\n\\n- **Fixed or Learnable:** Positional encodings can either be pre-defined using fixed functions like sine and cosine waves or learned parameters updated during training. Both approaches aim to provide meaningful positional signals.\\n- **Differentiable:** The encoding method must be differentiable to allow gradient-based optimization during backpropagation.\\n- **Compatibility with Input Embeddings:** Positional encodings are added or concatenated to the input embeddings, so they must match the dimensionality and scale of the token embeddings for seamless integration.\\n\\nTypically, the dimensionality of positional encodings aligns exactly with the embedding size used for tokens. For example, if each token is represented by a 512-dimensional vector, the positional encoding will also be a 512-dimensional vector. This alignment allows element-wise addition, preserving the embedding dimensions while augmenting them with position information. By combining token embeddings with positional encodings, transformers gain the ability to distinguish tokens based on their order, which is vital for understanding context in sequential data.\\n\\n## Sinusoidal Positional Encoding Explained\\n\\nThe original Transformer architecture uses sinusoidal positional encoding to inject information about the position of tokens within a sequence. Unlike recurrent or convolutional models, Transformers process all tokens simultaneously, so positional information must be explicitly added to the input embeddings.\\n\\nFor each position \\\\( pos \\\\) and dimension \\\\( i \\\\) of the positional encoding vector, the encoding is defined by sinusoidal functions as follows:\\n\\n\\\\[\\n\\\\text{PE}_{(pos, 2i)} = \\\\sin\\\\left(\\\\frac{pos}{10000^{\\\\frac{2i}{d_{\\\\text{model}}}}}\\\\right)\\n\\\\]\\n\\\\[\\n\\\\text{PE}_{(pos, 2i+1)} = \\\\cos\\\\left(\\\\frac{pos}{10000^{\\\\frac{2i}{d_{\\\\text{model}}}}}\\\\right)\\n\\\\]\\n\\nHere, \\\\( d_{\\\\text{model}} \\\\) is the dimensionality of the model embeddings, and the encoding alternates sine and cosine for even and odd dimensions respectively. This design creates a unique encoding for each position where each dimension corresponds to a sinusoid of different wavelength.\\n\\nMathematically, this formulation allows the model to learn to attend to relative positions because any linear function of position can be represented as a linear combination of these sinusoids. Furthermore, since the wavelengths range from \\\\( 2\\\\pi \\\\) to \\\\( 10000 \\\\times 2\\\\pi \\\\), the model can generalize to sequences longer than those seen during training by extrapolating the positional patterns.\\n\\nAdvantages of sinusoidal positional encoding include:\\n\\n- **Fixed Encoding**: The encoding is deterministic and does not require learning additional parameters, reducing model complexity.\\n- **Smoothness**: The continuous nature of sine and cosine functions ensures smooth transitions between positions, aiding gradient-based optimization.\\n- **Interpretability**: The encoding reveals relative position information naturally, as sinusoidal functions encode periodicity and distance in a way that the model can exploit for attention.\\n\\nHere is a minimal Python example to generate sinusoidal positional encodings:\\n\\n```python\\nimport numpy as np\\n\\ndef get_sinusoidal_positional_encoding(seq_len, d_model):\\n    pos = np.arange(seq_len)[:, np.newaxis]\\n    i = np.arange(d_model)[np.newaxis, :]\\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\\n    angle_rads = pos * angle_rates\\n\\n    # apply sin to even indices; cos to odd indices\\n    pos_encoding = np.zeros_like(angle_rads)\\n    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\\n    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\\n    return pos_encoding\\n\\n# Example usage\\npos_encoding = get_sinusoidal_positional_encoding(seq_len=50, d_model=512)\\nprint(pos_encoding.shape)  # (50, 512)\\n```\\n\\nThis approach ensures the Transformer model has a meaningful and generalizable representation of token positions without adding trainable parameters.\\n\\n> **[IMAGE GENERATION FAILED]** Sinusoidal positional encoding patterns for sequence positions and embedding dimensions.\\n>\\n> **Alt:** Heatmap visualization of sinusoidal positional encoding patterns across different positions and embedding dimensions\\n>\\n> **Prompt:** Generate a heatmap visualization showing sinusoidal positional encodings for a sequence of tokens. The x-axis should represent embedding dimensions alternating sine and cosine waves, and the y-axis should represent token positions. Use color gradients to illustrate the sinusoidal patterns clearly.\\n>\\n> **Error:** 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-flash-preview-image\\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-flash-preview-image\\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-flash-preview-image\\\\nPlease retry in 47.470718756s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-preview-image'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-preview-image'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-preview-image'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '47s'}]}}\\n\\n\\n## Learnable Positional Embeddings\\n\\nLearnable positional embeddings represent an alternative to fixed positional encodings by treating position information as parameter vectors that are learned during the model’s training process. Instead of using a predefined function like sinusoidal waves to encode position, the model initializes a set of embedding vectors—one per position up to a maximum sequence length. These embeddings are updated through backpropagation alongside the rest of the model parameters, allowing the network to optimize position representations specific to the task and data.\\n\\nCompared to sinusoidal encodings, learnable positional embeddings offer greater flexibility. Because they are not constrained to follow a fixed mathematical form, the model can adapt these embeddings to better capture positional dependencies that are relevant to the dataset. However, this flexibility comes at the cost of potentially reduced generalization to sequence lengths beyond those seen during training. Sinusoidal encodings, by contrast, provide a continuous and extrapolatable representation that can generalize to arbitrary sequence lengths without retraining.\\n\\nFrom an implementation standpoint, learnable positional embeddings are straightforward: they are typically implemented as an embedding matrix indexed by position. In frameworks like PyTorch or TensorFlow, this can be realized with an `nn.Embedding` or `tf.keras.layers.Embedding` layer. One consideration is to set a maximum sequence length, since the embedding matrix size is fixed and cannot easily handle longer sequences without retraining or resizing.\\n\\nModern transformer architectures often use learnable positional embeddings when the maximum sequence length is known and fixed, such as in language models trained on fixed-length inputs. This approach allows the model to fully leverage task-specific positional patterns. In contrast, models that require handling arbitrary sequence lengths or better extrapolation might prefer fixed sinusoidal encodings. Both methods remain widely used, and the choice depends on the specific application and performance trade-offs.\\n\\n## Edge Cases and Limitations of Positional Encoding Methods\\n\\nPositional encoding methods in transformers, whether fixed or learnable, face several challenges that can impact model effectiveness, especially in edge cases.\\n\\nFirst, learnable positional embeddings typically have a fixed maximum length determined during training. This limitation means that if the model encounters sequences longer than this length during inference, it lacks embeddings for those out-of-vocabulary positions. As a result, the model cannot accurately represent positions beyond its training scope, potentially degrading performance or requiring fallback strategies like truncation or interpolation.\\n\\nSecond, generalization to longer sequences than seen during training is a significant challenge. Fixed sinusoidal encodings theoretically extend to arbitrary lengths due to their continuous functional form, but in practice, their effectiveness can diminish as the sequence length grows far beyond training examples. Learnable embeddings, on the other hand, do not naturally generalize since they are discrete vectors tied to specific positions, making extrapolation impossible without additional mechanisms.\\n\\nFinally, positional encodings are susceptible to noise or corruption. If positional information is perturbed—due to input corruption, adversarial attacks, or implementation errors—the model's ability to discern token order can be impaired. This disruption can cause the attention mechanism to attend incorrectly, leading to degraded contextual understanding and performance drops. Robustness to such noise remains an open research area, with some approaches exploring regularization or alternative encoding schemes to mitigate sensitivity.\\n\\nOverall, while positional encodings enable transformers to capture sequence order, their fixed length, generalization limits, and vulnerability to noise highlight important practical considerations for developers designing and deploying transformer models.\\n\\n## Performance and Computational Considerations\\n\\nPositional encoding schemes directly influence the computational efficiency of transformer models during both training and inference. Fixed positional encodings, such as sinusoidal functions, introduce minimal overhead since they can be computed on the fly or precomputed once and reused without additional parameters. This results in a lightweight addition that does not significantly impact training speed or memory usage.\\n\\nIn contrast, learnable positional embeddings require storing additional parameters proportional to the maximum input length and embedding dimension. This increases the model’s memory footprint, especially for long sequences, impacting GPU memory consumption during training. Moreover, since these embeddings are updated via backpropagation, they add to the parameter optimization workload, potentially slowing down convergence slightly.\\n\\nThe choice between fixed and learnable encodings presents a trade-off: learnable embeddings can adapt to specific data distributions, potentially improving model expressiveness and accuracy. However, this comes at the cost of increased resource consumption and longer training times. Fixed encodings are more computationally efficient but may limit the model's ability to capture complex positional patterns.\\n\\nUltimately, the decision depends on application requirements. For scenarios demanding fast inference and lower resource usage, sinusoidal or other fixed encodings are preferable. For tasks where capturing nuanced positional relationships is critical, investing in learnable positional vectors may justify the additional computational cost.\\n\\n## Debugging and Observability Tips for Positional Encoding Issues\\n\\nWhen working with positional encoding in transformer models, effective debugging and observability are crucial to ensure the model correctly incorporates positional information.\\n\\n- **Visualizing Positional Encodings**: Plot the positional encoding vectors to inspect their patterns and variance across positions. Visualizations such as heatmaps can reveal whether the encodings follow expected sinusoidal patterns or learned embeddings exhibit meaningful diversity. Overlay positional encodings with token embeddings to verify how they combine visually.\\n\\n- **Testing Correct Addition of Positional Information**: Implement unit tests that confirm positional encodings are added element-wise to token embeddings before feeding into the transformer layers. For example, test with known inputs and positional indices to check if the resulting embeddings differ appropriately at different positions. Compare outputs with and without positional encodings to ensure positional information impacts downstream layers.\\n\\n- **Debugging Common Issues**:\\n  - *Sequence Length Mismatches*: Confirm that the positional encoding matrix matches the input sequence length. Mismatches can cause broadcasting errors or silent misalignments.\\n  - *Embedding Dimension Alignment*: Verify that positional encodings and token embeddings share the same dimensionality. A dimensionality mismatch will raise errors or lead to incorrect addition.\\n  - *Indexing Errors*: Check the correctness of position indices used when generating or applying encodings, ensuring zero-based or one-based indexing does not cause off-by-one errors.\\n  - *Unexpected Zero or Constant Vectors*: If positional encodings appear too uniform or zeroed out, investigate initialization and update logic, especially when using learned positional embeddings.\\n\\nBy combining visualization, targeted tests, and careful validation of dimensions and indices, developers can more quickly identify and resolve positional encoding issues in transformer implementations.\\n\\n> **[IMAGE GENERATION FAILED]** Key debugging and observability tips for positional encoding issues in Transformer models.\\n>\\n> **Alt:** Diagram summarizing debugging and observability tips for positional encoding in Transformers\\n>\\n> **Prompt:** Create a clear infographic summarizing key debugging tips for positional encoding in Transformer models, including visualizing positional encodings, testing addition to embeddings, and common issues like dimension mismatch and indexing errors. Use simple icons and short labels for each tip.\\n>\\n> **Error:** 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-flash-preview-image\\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-flash-preview-image\\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-flash-preview-image\\\\nPlease retry in 47.021329576s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash-preview-image', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-preview-image'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-preview-image'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '47s'}]}}\\n\"}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run(\"Positional Encoding in Transformer Architecture\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9022798",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchainvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
